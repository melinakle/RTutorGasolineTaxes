
# Problem Set: The Effects of Gasoline Taxes on Consumer Behavior

Author: Melina Klenk

#< ignore

```{r "0_1"}
#library(restorepoint)
# facilitates error detection
#set.restore.point.options(display.restore.point=TRUE)

library(RTutor)
library(yaml)
setwd("~/Melina/Masterarbeit/Gasoline Taxes")
ps.name = "Gasoline_Taxes"; sol.file = paste0(ps.name,"_sol.Rmd")
libs = c("dplyr", "ggplot2","ggthemes", "choroplethr", "lfe", "gridExtra", "stargazer", "scales", "forcats") # character vector of all packages you load in the problem set
name.rmd.chunks(sol.file) # set auto chunk names in this file

create.ps(sol.file=sol.file, ps.name=ps.name, user.name=NULL,libs=libs, stop.when.finished=FALSE, addons="quiz")
show.shiny.ps(ps.name, load.sav=FALSE,  sample.solution=TRUE, is.solved=TRUE, catch.errors=TRUE, launch.browser=TRUE)
stop.without.error()


 rtutor.package.skel(sol.file=sol.file, ps.name=ps.name,libs=libs,
    pkg.name="RTutorGasolineTaxes",   # Name of the problem set package
    pkg.parent.dir = "D:/libraries/RTutorExample", # Parent directory 
    author="Melina Klenk", # Your name
    github.user="melinakle",     # Your github user name
    extra.code.file=NULL, # name of extra.code.file
    var.txt.file=NULL,    # name of var.txt.file
    overwrite=FALSE  # Do you want to override if package directory exists?
  )
                  
```
#>

## Exercise Overview

Welcome to this interactive analysis, which is part of my master's thesis at Ulm University. During the following exercises, we will analyze the effects of gasoline taxes on consumer behavior based on the article ["Gasoline Taxes and Consumer Behavior"] (https://www.aeaweb.org/articles/pdf/doi/10.1257/pol.6.4.302) by Shanjun Li, Joshua Linn and Erich Muehlegger (2014). In the course of the problem set, it will be referred to as "Li et al." or "the paper". The purpose of the paper is to examine how gasoline taxes affect gasoline consumption as distinct from tax-inclusive or tax-exclusive retail gasoline prices. 

Facing a global climate crisis, gasoline taxes are a vividly discussed topic in politics and the society. According to the United States Environmental Protection Agency, the transportation sector was responsible for around 29% of total U.S. Greenhouse Gas Emissions in 2019, representing the largest emission source. Over half of these transportation emissions result from passenger cars and trucks, which are petroleum based and either use diesel or gasoline (cf. EPA, 2019). Private transportation is of high importance to the U.S. economy and reducing gasoline demand by increasing the gasoline taxation could lead to large $CO_{2}$ savings. However, raising gasoline taxes is a quite unpopular political measure because of the importance of private transportation (cf. Tiezzi and Verde (2016), p. 72). 

Do the effects of gasoline taxes on the consumer behavior outweigh its negative standing in public? Li et al. (2014) state three main reasons for the use of gasoline taxes in order to reduce automobile use. The first one is the correction of externalities associated with motorized private transport. Those include carbon dioxide emissions, local air pollution, traffic accidents and traffic congestion (Parry et al., 2007). Furthermore, gasoline taxes are aimed to raise government revenue. The third reason for the reduction of petroleum-based automobile use is to reduce the dependency on oil imports. Although, in 2020, the U.S. petroleum production exceeded the petroleum consumption, crude oil and other petroleum products were still imported to cover the domestic demand and to deliver international markets (cf. EIA, 2021a). Moreover, the transportation sector is still the largest user of petroleum based products (cf. EIA, 2021b). 

But how and to which extent do gasoline taxes affect the behavior of consumers? The authors found that gasoline tax changes indeed lead to stronger consumer responses than commensurate changes in the tax-inclusive gasoline prices. To derive their results, Li et al. distinguished between gasoline taxes and tax-exclusive gasoline prices and measured their effects on different response variables: gasoline consumption, vehicle miles traveled and vehicle fuel economy. 

You can now start on your data driven journey to analyze the findings and implications of Li et al.'s research on the effects of gasoline taxes. Most results will be derived by using the programming language R, so entering your own R code from time to time will be expected. A number of R functions will be explained as needed, but basic R skills are necessary to solve this problem set. For an introduction to programming in R you can have a look at [R for Beginners](https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf).  Below, you can find more information on how to solve the problem set. If you are ready, proceed to *Exercise 1* and get started!


### Contents


$\qquad$  Overview
  
$\qquad$  1 Examining U.S. Gasoline Prices and Taxes

$\qquad$ $\qquad$ 1.1 Data Overview and Summary Statistics

$\qquad$ $\qquad$ 1.2 Plotting the Data

$\qquad$ $\qquad$ 1.3 Visualization with Choropleth Maps

$\qquad$  2 Causal Effects of Gasoline Taxes on Retail Gasoline Prices

$\qquad$  3 Effects of Gasoline Taxes on Consumer Behavior: Aggregate State Level Analysis

$\qquad$  4 Robustness Check

$\qquad$  5 Effects of Gasoline Taxes on Consumer Behavior: Household Data Analysis

$\qquad$ $\qquad$ 5.1 The Effects on Vehicle Purchase - MPG

$\qquad$ $\qquad$ 5.2 The Effects on Vehicle Driving - VMT

$\qquad$  6 Explanations of Findings

$\qquad$ $\qquad$ 6.1 Persistence

$\qquad$ $\qquad$ 6.2 Salience

$\qquad$  7 Conclusion

$\qquad$ References


### Outline


In *Exercise 1*, you are introduced to some basic principles of our analysis and the composition of retail gasoline prices over time. Furthermore, we examine a data set on gasoline taxes on the federal and state level and see how the tax rate differs between the different U.S. States. In *Exercise 2*, we perform a linear regression of gasoline retail prices and examine the tax incidence. *Exercise 3* contains the main results. Data of the gasoline taxes and prices is combined with data about consumer behavior for an aggregate state-level analysis. We analyze the effects of gasoline taxes and tax-exclusive prices on consumer behavior with the help of linear regression modeling.  In *Exercise 4*, we perform an additional robustness check in order to test the exogeneity of gasoline taxes as explanatory variable. *Exercise 5* deals with the household-level analysis, where we examine the effects of gasoline taxes on the decision of vehicle purchases in *Exercise 5.1* and vehicle driving in *Exercise 5.2*. An introduction of two explanations of our findings in previous exercises, takes place in *Exercise 6*. In this context, we first focus on persistence in *Exercise 6.1* before we take a closer look at the salience of gasoline tax changes in the media in *Exercise 6.2*. Finally, in *Exercise 7*, we draw a conclusion and summarize our results. 


### How to solve this problem set


The following elements are part of this problem set and help to run the analysis and deepen your knowledge: 


- Tasks: Whenever you are required to interact with a code chunk, the text, which succeeds "**Task:**" will tell you what you have to do.

- Code Chunks are used to enter and run R code. You are ought to solve them in the displayed order. Each code chunk includes the following buttons: 

    + `Edit`: At the beginning of each exercise, you first need to press this button in order to enter your R code into the first code chunk. 
    + `Check`: After entering your solution, press check to run your code and see whether your answer is correct. In some code chunks, the correct answer is already predetermined, then you only need to press check to solve it.
    + `Hint`: Press hint, if you need help to solve the code chunk. 
    + `Run chunk`: This button runs the code chunk without checking whether it is correct.
    + `Data`: This button is a link to the data explorer, in which you can view data sets and have a look at variable descriptions.
    + `Solution`: Pressing this button enters the sample solution into the code chunk.

- Quizzes are not mandatory for continuing with the problem set, but you can check your acquired knowledge. 

- Info Boxes offer additional information during the problem set, for example explanations of R functions or mathematical proofs.

To navigate through the problem set, you can use the button `Go to next exercise...` at the bottom of the page or click through the menu bar at the top. It is possible to solve each exercise without solving the previous ones, however you are encouraged to solve the exercises in the specified order as they follow a didactic structure and frequently use data sets that have been derived and analyzed in previous exercises.


## Exercise 1 -- Background on U.S. Gasoline Prices and Taxes
<style>img{max-width:98%;}
</style>

In the first exercise, we will investigate variations of U.S. gasoline taxes and retail prices over time and in different states. Therefore, we will take a closer look at the first data set and explore its structure. The data set contains annual, state level data and is provided by Li et al. The authors assembled data from multiple sources. Quantities and prices were retrieved from the Energy Information Administration whereas tax, travel and highway infrastructure data came from annual issues of the Highway Statistics Annual published by the Federal Highway Administration. The demographic data was collected from the Census Bureau and the Bureau of Economic Analysis and political variables from publicly available sources. The data frame covers all U.S. states except for Alaska, Hawaii and the District of Columbia for the years 1966 - 2010. This data base is also used in *Exercise 2,3 and 4*. Note that the included data is restricted to what is needed per exercise, though. 

## Exercise 1.1 -- Data Overview and Summary Statistics

### Data Overview


Let's have a look at the actual data. 

**Task:** Use the `readRDS()` command to load the data set and store it in a variable called `adat`. As part of the code is already given, you just need to replace the ___ with the variable name.

#< info "readRDS() and saveRDS()"
The command `readRDS()` reads a RDS file and stores it in a variable with the determined name. If the file is within your working directory, it suffices to use the name of the file as an argument.


```{r "3_1",eval=FALSE}
dat <- readRDS("data.rds")
```

Furthermore, you can save R objects as RDS files using the `saveRDS()` command.

```{r "3_2",eval=FALSE}
saveRDS(dat, file="data.rds")
```

If you want more detailed information on the syntax and usage of the commands, call `help(readRDS)`.
#>

```{r "3_3"}
#< fill_in
___ <- readRDS("annual_data.rds")
#>
adat <- readRDS("annual_data.rds")
#< hint
display("You just have to replace ___ with the correct variable name.")
#>
```

#< award "First Task Completed!"
Congratulations, you successfully solved the first code chunk. Throughout the problem set, you can earn a number of awards. After completing the last exercise, you can see a summary of all your achieved awards.
#>

Now that the data is loaded, we want to take a closer look at its structure and variables. There are several commands in R, which help us to access the data. We will use the command `head()` in order to get familiar with the variables included in our data frame. This command will show us the first six rows of our data frame.

**Task:** Show the first six rows of the data frame `adat` using the `head()` command. Again, the basic structure of the command is already given.

```{r "3_4"}
#< fill_in
___(adat)
#>
head(adat)
```

As we can see, our data frame consists of eight columns: the `fips` code of the state, the state's name, the `year` of the observation, the federal gas tax `fgastax`, the state gas tax `sgastax`, the sum of federal and state gas taxes `gas_tax_all`, the nominal oil price `nom_oilprice` and the tax-inclusive gas price `taxin_gas_price`. The values of the taxes and oil prices are provided in cents per gallon (cpg). Note, that in the column of the nominal oil price, data is missing for some years - characterized by NA, which stands for *not available*. Each row represents the data for a certain state per year. 


### Summary Statistics


In order to get a feeling for the height of the gasoline taxation in the U.S., let's compute some summary statistics of the state and federal gasoline taxes. With the next code, we would like to calculate the mean, maximum and minimum state tax over the period of our data set. Additionally, the development of the federal gas tax per year will be shown. 

To do so, we are using the `dplyr` package, which delivers a helpful set of functions for data manipulation and transformation. Furthermore, we will use the so-called `pipe operator` to facilitate the work with the different commands.

#< info "group_by() and summarize()"
The command `group_by()` of the `dplyr` package converts an existing data frame by forming groups of the specified variables. After the grouping, the following commands are performed for each group separately. Therefore, the `group_by()` command is often combined with other calculations or commands of the `dplyr` package.

One command, which is usually following the `group_by()` function, is called `summarize()`. It reduces multiple values down to a single summary. For example, if data is grouped by state, the data frame now only shows the calculated values, such as the mean, per state. Furthermore, it can be determined, which summary statistics of the new groups should be calculated and how the corresponding columns should be named. 

If you want more detailed information on the syntax and usage of the commands, call `help(group_by)` or `help(summarize)`.
#>

#< info "Pipe Operator %>%"
A pipe operator is very useful to write clear and easy code because it links several commands. It will forward the result of an expression into the next function call by adding the pipe `%>%` at the end of a command. By that, the first argument of each function that is usually the data on which to perform the operation, is omitted. Thus, the code gets shorter. 

Combined with the `group_by()` and `summarize()` functions a typical code chunk with the pipe operator can look like the following. If you have, for example, a data frame `dat` about the inhabitants of a country with the columns `state` and `age`, and you first want to group the data frame by state and then compute the average age of the inhabitants per state, you could use the following command chain:

```{r "3_5",eval=FALSE}
dat %>%
  group_by(state) %>%
  summarize(average_age = mean(age))
```

#>

**Task:** First, assign the new data frame to the name `sumdat`. Then, use the `group_by()` command to group the data set by the variable `year`. Then, use the `summarize()` function to create a table, which shows the `mean()`, `min()` and `max()` of the variable `sgastax` and the mean of `fgastax` for each year. Connect the commands by using the `pipe operator`. Fill in the blank spaces.

```{r "3_6"}
#< fill_in
library(dplyr)
___ <- adat %>%
  ___ %>%
  summarize(avg_sgastax = mean(sgastax), min_sgastax = ___ , 
            max_sgastax = ___, avg_fgastax= ___)
#>
library(dplyr)
sumdat <- adat %>%
  group_by(year) %>%
  summarize(avg_sgastax = mean(sgastax), min_sgastax = min(sgastax), 
            max_sgastax = max(sgastax), avg_fgastax=mean(fgastax))

#< hint
display("First, assign the right name of the new data frame. Then, use the group_by command. The following gaps are supposed to be replaced by the calculations of the new variables. Use the calculcation of the avg_sgastax as example.")
#>
```

#< award " Data Transformer!"
Well done! You transformed your first data set. As this is a task that will be performed repeatedly, you are well prepared for the next exercises and code chunks.
#>

**Task:** Show five rows of `sumdat` with the `slice_sample()` command. Just add the name of the data frame and the number of rows into the bracket of the command. 

#< info "slice()"
In the `dplyr` package, there are a number of different commands that show a specific part of the given data frame. 
First, we are going to use `slice_sample()`, which shows a random selection of rows. Later on, we will also use `slice_min()` and `slice_max()`, which select rows with highest or lowest values of a variable.

If you want more detailed information on the syntax and usage of the commands, call `help(slice)`.
#>

```{r "3_7"}
#< fill_in
slice_sample(___, n=_)
#>
slice_sample(sumdat,n= 5)

#< hint
display("Fill in the name of the data frame in the first gap. In the second gap, add the number of rows, that we want to display.")
#>
```

As is visible, `slice_sample` shows a random selection of rows of the given data frame. In the new data frame `sumdat`, each row represents one year. For each year, we can see the average, minimum and maximum state tax as well as the average federal tax. For a better overview, we will plot this data in the following exercise. 

## Exercise 1.2 -- Plotting the Data

**Task:** Press `edit` first. Then, use the `readRDS()` command to load the data sets `annual_data.rds` and `sum_data.rds`. Store them in a variable called `adat` and a variable called `sumdat`. The basic structure of the command is already given in the code chunk.

```{r "4_1"}
#< fill_in
___ <- readRDS("annual_data.rds")
___

#>
adat <- readRDS("annual_data.rds")
sumdat <- readRDS("sum_data.rds")
#< hint
display("You just have to replace ___ with the correct variable names.")
#>
```

To visualize and compare the computed values over time, we can plot them. The `ggplot2` package offers a number of varieties to create elegant and versatile plots. 

#< info "ggplot() and ggthemes()"

`ggplot()` allows us to visualize data with only a few steps. 
First, we need to specify the data that is going to be used (`adat`) and the x-axis: ggplot(data= adat, aes(x=year)). As we want to plot different values on the y-axis, we need to add them separately with a `+`. For example +(aes(y=meansgastax)). Now we can add functions to the background just by using the `+` sign. For example, we can add a line with the command `geom_line()` or single data points with `geom_point()`. `ggplot()` offers a variety of possibilities to alter the plot to one's own liking by adding labels (+labs(title="___")), or themes (+theme()) or by customizing the scale.

`ggthemes()` offers a number of extra themes, scales and functions for ggplot.

If you want more detailed information on the syntax and usage of the commands, call `help(ggplot)` or `help(ggthemes)`. 
#>

**Task**: Load the packages `ggplot2` and `ggthemes` with the `library` command. 

```{r "4_2"}
#< task

#>
library(ggplot2)
library(ggthemes)

```

**Task**: Our graph is based on the data frame `sumdat` and the `year` is drawn on the x-axis. Fill in the missing information, then press *check* to have a look at the resulting graph. 

```{r "4_3",fig.width=8.5}
#< fill_in
ggplot(data=___, aes(x=___))+
       geom_line(aes(y=avg_fgastax, color="1"))+ 
       geom_line(aes(y=avg_sgastax, color="2"))+
       geom_line(aes(y=max_sgastax, color="3"))+
       geom_line(aes(y=min_sgastax, color="4"))+
       theme_minimal()+
       labs(x="Year", y="Cents/gallon", 
       title= "State and Federal Gasoline Taxes")+
       scale_color_manual(values=c("orange","blue","darkgreen", "red"), 
       name=" ",
       labels=c("Federal gas tax",
                "Mean state gas tax",
                "Max state gas tax", 
                "Min state gas tax"))
    
    
#>

ggplot(data=sumdat, aes(x=year))+
        geom_line(aes(y=avg_fgastax, color="1"))+
        geom_line(aes(y=avg_sgastax, color="2"))+
        geom_line(aes(y=max_sgastax, color="3"))+
        geom_line(aes(y=min_sgastax, color="4"))+
        theme_minimal()+
        labs(x="Year", y="Cents/gallon", 
        title= "State and Federal Gasoline Taxes")+
        scale_color_manual(values=c("orange","blue","darkgreen", "red"), 
        name=" ",
        labels=c("Federal gas tax", 
                 "Mean state gas tax",
                 "Max state gas tax", 
                 "Min state gas tax"))
        
    
#< hint
display("Replace the first placeholder with the name of the data frame and the second one with the year.")
#>
```
As we can see, the `average state gas tax` (in blue) rises slowly over time. However, state taxes rise much more quickly in some locations than in others. When we look at the `maximum state gas tax` (in green) compared to the `minimum state tax` (in red), it is visible that the difference between states increases over time. In 1966, the difference between the highest and lowest state tax was only $2.5$ cpg. In 2008, the lowest state tax lies at $7.5$ cpg whereas the highest state tax lies at $37.5$ cpg - a major difference of $30$ cpg. 

The `federal gas tax` (in orange) started at $4$ cpg in 1966 and passed three large increases. In 1983, it was raised to $9$ cpg. Seven years later, in 1990, the amount rose to $14.1$ cpg. Since 1993 until 2008, the federal gas tax remained at a level of $18.4$ cpg. Interestingly, the value of the federal gas tax is lower than the mean state gas tax throughout the depicted period. 

*This figure can be found as Figure 4 at page 310 in the paper.*

#< quiz "Height of Federal Gas Tax Today"
question: Our data shows that the federal gas tax was increased to 18.4 cpg in 1993. How high is the federal gas tax in cents per gallon today? Just type in the amount you guess with one decimal.
answer: 18.4
#>


#< quiz "Height of Federal Gas Tax Comparison"
question: Compared to other industrial countries, do you think the U.S. has a high or low gasoline taxation? 

sc:
- low*
- high

#>

Actually, the federal gas tax increase in 1993 was the last one until today. Compared to other industrial countries, the gasoline tax in the U.S. is one of the lowest. In the 2020 Consumption Tax Trends Report of the OECD, the combined average gasoline tax of the federal and state tax in the U.S. is the second lowest of all OECD countries, only Mexico's tax was lower. The share of the total tax of the total price in the U.S. was at only $18.6%$ in 2019, whereas in most European Countries this value was above $50%$. In Germany, for instance, the total tax corresponded to $61.3%$ of the the total price (cf. OECD (2020), p. 198). 

To see the historical development of the U.S. gas tax, it would be very interesting to graph the distribution of the tax fraction over time compared to the absolute course of the median tax-inclusive price. 

In order to do so, we need to add another column to our data frame, where we calculate the tax fraction. We can easily execute this task by using the `mutate()`  function of the `dplyr` package. 

#< info "mutate()"
`mutate()` adds new variables to our data frame. As input, the command needs the new name of the respective variable and the function to calculate it. Moreover, `mutate()` can be combined with `group_by()` to calculate new values for specific groups. 

For further details, call `help(mutate)`. 
#>

**Task:** Add a new column called `tax_fraction` to calculate the tax fraction of the tax-inclusive gas price using the `mutate()` command on the `adat` data frame. Then show the first rows of `adat_fraction` with the `head` command. The basic structure of the command is already given.

```{r "4_4"}
#< fill_in
adat_fraction <-adat %>%
  mutate( ___ =((fgastax+sgastax)/taxin_gas_price))

___
#>
adat_fraction <- adat %>%
  mutate(tax_fraction=((fgastax+sgastax)/taxin_gas_price))

head(adat_fraction)

#< hint
display("Fill in the name of the new column. Then use the head() command to show the new data frame adat_fraction.")
#>
```

Our initial data frame `adat` was extended by one new column: the tax-fraction of the tax-inclusive retail price per state and year. Now, we would like to analyze the fifth and ninety-fifth percentile of the tax fraction as well as the median tax fraction. For comparison, we further calculate the median tax-inclusive gas price over time. Again, we can use the `group_by()` and `summarize()` commands. 

#< info "Median and Percentiles"

If you arrange the values of a numeric variable in an ascending order, the median is the value where exactly the same number of values in the data series are higher and lower. If the number of observations is odd, the median represents the value, which is directly in the middle of the ascending data series. If the number of observations is even, the median is calculated as the arithmetic mean of the two middle values. The median can also be called the 50th percentile. 

Other percentiles or quantiles can be calculated similarly. At the fifth percentile, 5% of the observations are below and 95% above. This is reversed for the 95th-percentile, where 95% of the observations are below and 5% of the observations are above the searched value (cf. Perret (2019), pp. 52-53). 

In R, the command `quantile()` calculates the different percentiles. If you would like to calculate the median of a given vector x for example, and NA values are supposed to be omitted before the calculation, the code would look like this: 

```{r "4_5",eval=FALSE}
quantile(x, probs= 0.05, na.rm=TRUE)
```

By changing the probability, different percentiles can be calculated easily. Note that, the part `na.rm=TRUE` is added to omit NA-values before the calculation.

#>

**Task:** Use the `group_by()` command to group the data per year. Then use `summarize()` to calculate the fifth percentile, the median and the ninety-fifth percentile of the tax fraction as well as the median of the tax-inclusive gas price. Assign the new table to the variable `quantdat`. The basic structure of the command is already given.

```{r "4_6"}
#< fill_in
quantdat <- adat_fraction %>%
  ___ %>%
  summarize(fifth_perc=quantile(tax_fraction, probs=0.05, na.rm=TRUE),
            median=median(tax_fraction, na.rm=TRUE), 
            ninetyfifth_perc=quantile(tax_fraction, probs= ___ , na.rm=TRUE),
            median_price=___ (taxin_gas_price, na.rm=TRUE)/1000))

#>
quantdat <- adat_fraction %>%
  group_by(year) %>%
  summarize(fifth_perc=quantile(tax_fraction, probs=0.05, na.rm=TRUE),
            median=median(tax_fraction, na.rm=TRUE),
            ninetyfifth_perc=quantile(tax_fraction, probs=0.95, na.rm=TRUE),
            median_price=median(taxin_gas_price, na.rm=TRUE)/1000)

#< hint
display("First use the group_by command to group by year. For the placeholder after probs=___, look at the example for the fifth percentile and apply it to the ninety-fifth percentile. For the last gap, use the median command.")
#>
```

**Task**: Show the first few rows of `quantdat` with the help of the `head` command. 

```{r "4_7"}
#< task

#>
head(quantdat)
```

As you can see, the new table only shows the previously calculated values of the percentiles and the median of the tax-fraction as well as the median of the tax-inclusive gas price per year. 

To get an overview, we visualize the data with the help of `ggplot()` again. 

**Task**: Just press *check* to load the graph of the tax fraction of the retail price by year. Note that the value of the median tax-inclusive gas price is divided by 1000 as it is an absolute value and we want to compare its development in the same graph as that of the relative tax fraction. Furthermore, a second y-axis is pictured. 

```{r "4_8",fig.width=8.5}
#< task_notest
ggplot(data=quantdat, aes(x=year))+
         geom_line(aes(y=fifth_perc, color="1"))+
         geom_line(aes(y=median, color="2"))+
         geom_line(aes(y=ninetyfifth_perc, color="3"))+
         geom_line(aes(y=median_price, color="4"))+
         labs(x="Year", y="Tax fraction of retail price", 
         title= "Distribution of Tax Fraction over Time")+
         scale_color_manual(values=c("grey","black","grey", "orange"), 
         name=" ",
         labels=c("Fifth percentile", 
                  "Median tax fraction",
                  "Ninety-fifth percentile", 
                  "Median tax-inclusive gas price"))+
         theme(legend.position = "bottom")+ 
         scale_y_continuous(sec.axis = sec_axis(~ . * 1000, 
         name="Gasoline price (cpg)"))+theme_minimal()
#>
```

It is striking that the development of the median tax-fraction runs almost opposite to the median tax-inclusive retail price. Overall, taxes make up around $26$% of the after-tax price at the median. However, this number varies substantially over time and across states. In the late 1960s and 1990s, taxes were relatively high and met comparably low oil prices. Vice versa, during the early 1980s and after 2005, oil prices rose whereas the tax proportion was low. The tax-fraction reached a peak in 1998, when it was at approximately $36.7$% at median. The ninety-fifth percentile even reached the $40$% barrier, then. Afterwards, market gasoline prices rose, whereas the tax fraction decreased. 

*This figure corresponds to Figure 5 at page 311 of the paper.*

As already mentioned, the state gasoline taxes were changed with very different frequency and magnitudes. For this reason, we want to find out, how much the state taxes were increased during two periods: from 1966 to 1987 and from 1987 to 2008. Again, we use the `dplyr` functions `filter()`, `select()` and `mutate()` in order to prepare our data frame. This time, it is also necessary to calculate the values of the tax increases between the years, as they are not part of our data frame yet. For this matter, we use the `lag()` command. Another newly introduced command we are using from the `dplyr` package is the `arrange()` command. 


#< info "lag() and lead()"
`lag()` computes a lagged version of a time series, shifting the time base back by a given number of observations. For instance, if you have monthly data arranged in ascending order, you can use the following call to get the variable of a year ago:

```{r "4_9",eval=FALSE}
dat <- lag(income, 12)
```

The shown command gives out the value of the income from 12 months ago. The only arguments the function needs are the name of the variable and the number of lags in units of observations. The same calculation can be done in the opposite direction with the command `lead()`, which shifts the time base forward instead. 

If you want more detailed information on the syntax and usage of the command, call `help(lag)` or `help(lead)`.
#>

#< info "arrange()"
The `arrange()` command out of the `dplyr` package changes the order of the rows of a data frame by sorting them by the values of specific columns. As an input, it needs the name of the respective column or several columns. 

```{r "4_10",eval=FALSE}
dat <- arrange(income)
```

If you want more detailed information on the syntax and usage of the command, call `help(arrange)`.
#>

**Task:** Use the `filter()` command to only keep the years 1966, 1987 and 2008 in our data frame `adat`. Assign the new data frame to the variable `tax_increase`. Then arrange the data frame after `state` and `year`. Note that part of the code is already given. Then show the first few rows with the `head` command.

```{r "4_11"}
#< fill_in
___ <- adat %>%
  filter(year=="1966"| year=="___"| ___ ) %>%
  arrange(state, ___)

head(tax_increase)
#>
tax_increase <- adat %>%
  filter(year=="1966"| year=="1987"| year=="2008") %>%
  arrange(state, year)

head(tax_increase)
#< hint
display(" Replace ___ with the correct commands. Take a look at the given code if you are unsure.")
#>
```

#< award "dplyr Expert"
Well done! You already master some very important `dplyr` functions, such as: filter(), select(), rename(), mutate() or arrange() and can competently transform data sets. 
#>

Great, now we have a data frame arranged by state and year with only the relevant years for our next task. In the next step, we need to create new columns, where we calculate the tax increases from 1966 to 1987 and 1987 to 2008. To facilitate the calculation, we also add a column with the growth from 1966 to 2008. Note how the `lag()` function helps us to get the right results. In addition, our results are rounded. 

#< info "round()"
The `round`()` command of base R is, as the name suggests, rounding numeric vectors to the number of decimals that is specified. As an input, it needs the name of the value that is ought to be rounded and the number of decimals that should be displayed.

```{r "4_12",eval=FALSE}
round(x, digits=2)
```

If you want more detailed information on the syntax and usage of the command, call `help(round)`.
#>

**Task:** Just press *check* to create the new columns.

```{r "4_13"}
#< task_notest
tax_growth <- tax_increase %>%
  mutate(growth8708=if_else(year=="2008",round(sgastax-lag(sgastax),1),0),
         growth6608=if_else(year=="2008",round(sgastax-lag(sgastax,2),1),0),
         growth6687=if_else(year=="2008", growth6608-growth8708,0 )) %>%
  filter(year=="2008")
#>
```

Excellent, with our new columns we can easily find out, which states had the highest and smallest tax increases over our observed period. 
To carry out this task, we use two more commands out of the `dplyr` package: `slice_min()` and `slice_max()`. 

**Task:** Use the `slice_min()` command to find the three states with the smallest state tax increase from 1966 to 2008 of our data frame `tax_increase`. The command is based on the column `growth6608`.

```{r "4_14"}
#< fill_in
slice_min(tax_growth, ___ ,n= ___)
#>
slice_min(tax_growth, growth6608,n=3)
#< hint
display(" Insert the name of the column and the number of rows after the n=.")
#>
```

Georgia, New Jersey and Florida are the states with the smallest state tax increases from 1966 to 2008. Georgia's state tax was only increased by $1$ cpg over the whole period, whereas New Jersey's and Florida's tax was increased by $4.5$ cpg and $8.6$ cpg, respectively. 

Which states had the highest increase, though? 

**Task:** Use the `slice_max()` command to find the three states with the highest state tax increase from 1966 to 2008 of our data frame `tax_increase`. The command is based on the column `growth6608`.

```{r "4_15"}
#< fill_in
slice_max(tax_growth, ___ ,n= ___)
#>
slice_max(tax_growth, growth6608,n=3)
#< hint
display(" Insert the name of the column and the number of rows after the n=.")
#>
```

Washington, West Virginia and Wisconsin represent the states with the highest state tax increases. The taxes rose by $30$ cpg, $25.2$ cpg and $23.9$ cpg from 1966 to 2008. 

The revenue of the state gas taxes is used for transportation infrastructure maintenance and new projects. Due to a number of reasons, such as changes in the population, business activity or aging structures, a gap between the infrastructure needs and actual funds can arise. This then leads to a debate about changing the state gasoline tax, which usually takes several years until an increase is implemented (cf. Li et al. (2014), p. 310). 

In order to get an overview of the state gasoline tax increases, we can visualize our data again with `ggplot`. We can investigate whether the overall tax increases have been higher from 1966 to 1987 or from 1987 to 2008 and see which states have had the highest and lowest increases. 

**Task:** Just press *check* to create the two plots that depict tax increases during the two periods.

```{r "4_16",fig.width=8.5, fig.height=6}
#< task_notest
library(forcats)

increase6687 <- tax_growth %>%
  select(state, growth6687)%>%
  mutate(state= fct_reorder(state,growth6687))
  
g1 <- ggplot(data=increase6687, aes(x=state))+
        geom_col(aes(y=growth6687))+
        coord_flip()+theme_minimal()+
        labs(y="Cents/Gallon", x="State", 
        title= "Gasoline Tax Increase per State 1966-1987")

increase8708 <- tax_growth %>%
  select(state, growth8708)%>%
  mutate(state= fct_reorder(state,growth8708))
  
g2 <- ggplot(data=increase8708, aes(x=state))+
        geom_col(aes(y=growth8708))+
        coord_flip()+
        labs(y="Cents/Gallon", x="State", 
        title= "Gasoline Tax Increase per State 1987-2008")+ theme_minimal()
#>
```


Now, we want to look at both graphs simultaneously, which is possible with the help of the package `gridExtra` and the command `grid.arrange()`. You'll find more details in the *info box* below. 

#< info "grid.arrange()"

With the help of the `gridExtra` package it is possible to easily layout a number of plots next to or above each other. The function we use is called `grid.arrange()`. In order to show the plots, you just need to add their variable names within the brackets. 

For more info call `help(grid.arrange)`. 

#>

**Task**: First load the `gridExtra` package with the `library` command. Then, specify the names of the plots, we want to see `g1` and `g2`.

```{r "4_17",fig.width=10, fig.height=6}
#< fill_in
___
grid.arrange(___,___, ncol=2)
#>
library(gridExtra)
grid.arrange(g1,g2, ncol=2)

```

#< award "Lots of Plots"
Great, you became familiar with different visualization options with `ggplot2`. Moreover, you can use them to create your own plots. 
#>

Comparing the two bar charts of the different periods shows that the state tax increases were larger during the later period from 1987 to 2008. The x-axis of the first plot is labeled until the value $10$ whereas the other x-axis is labeled until $20$. From 1966 to 1987, the highest tax increase amounted to $14$ cpg in Montana, followed by Utah, Connecticut and Wisconsin with $13$ cpg, whereas the highest increase from 1987 to 2008 totaled to $19.5$ cpg in Washington. Pennsylvania, West Virginia and New York also had increases of more than $15$ cpg. Both, in Virginia and Georgia, the state gasoline tax was not increased at all from 1987 to 2006.

## Exercise 1.3 -- Choropleth Maps

**Task**: Use the `readRDS()` command to load the data set and store it in a variable called `adat`. The basic structure of the command is already given in the code chunk.

```{r "5_1"}
#< fill_in
___ <- readRDS("annual_data.rds")
#>
adat <- readRDS("annual_data.rds")
#< hint
display("You just have to replace ___ with the correct variable name.")
#>
```

We have seen that the state tax increases and consequently the tax fraction of gasoline retail prices vary substantially across states. Hence, it would be of interest to see a comparison of the absolute state gasoline taxes at different times. The command `state_choropleth()` out of the `choroplethr` package is an easy way to depict the respective values per state on the U.S. map. 

#< quiz "tax increase"
question: Before looking at the data of the absolute gasoline taxes per state - which state do you think has had the highest gasoline taxation in 2008? 

sc:
- Washington*
- California
- Texas

success: Great, your answer is correct!
failure: Try again.
#>

In preparation for the choropleth map, we use some more commands from the `dplyr` package: `filter()`,  `select()` and `rename()`. 

#< info "filter()"
The command `filter()` generates a subset of a data frame by only keeping the rows or cases for which a certain condition is met. 

If we, for example, are only interested in data before the year 2009, we could use the following code: 

```{r "5_2",eval=FALSE}
dat <- filter(dat, year<2009)
```

If you want more detailed information on the syntax and usage of the command, call `help(filter)`.
#>

#< info "select()"
With the `select()` command you can choose, which columns you want to keep in a data frame by their name. On top of that, you can change the order of the respective columns as they are aligned as they are called in the command. 
As an example, consider a data frame `dat` with three columns `state`, `inhabitants` and `size`. If you are only interested in the states and their sizes, you could use the following code:

```{r "5_3",eval=FALSE}
select(dat, state, size)
```

If you want more detailed information on the syntax and usage of the command, call `help(select)`.
#>

#< info "rename()"
With the command `rename()` you can easily change the names of columns in a data frame. All you need to do is to equate the new name with the old name: 

```{r "5_4",eval=FALSE}
dat <- rename(dat, newname=oldname)
```

If you want more detailed information on the syntax and usage of the command, call `help(rename)`.
#>

#< info "state_choropleth()"
The command `state_choropleth()` from the `choroplethr` package is based on a map of the U.S. The states are shaded in proportion to the displayed variable. As an input, `state_choropleth()` requires a data frame with only two columns: the first one is called `region` and contains the state names in lower caps. The second one is called `value` and contains the variable you want to visualize. You can specify a title for the map by using the argument *title="..."* and a name for the legend by using *legend="..."*.

If you want more detailed information on the syntax and usage of the command, call `help(state_choropleth)`.
#>

**Task:** First, we want to visualize the state gas tax in the year 1966 by creating a new data frame, where we filter for the year 1966 and select the columns `state` and `sgastax`. Then, we rename them as `region` and `value`.
Just fill in the gaps in the code chunk. 

```{r "5_5"}
#< fill_in

# Data Preparation
state_1966 <-adat %>%
  filter(year=="___") %>%
  select(___,___) %>%
  rename(___=state, ___=sgastax)

#>

# Data Preparation

state_1966 <-adat %>%
  filter(year=="1966") %>%
  select(state,sgastax) %>%
  rename(region=state, value=sgastax)

```

**Task**: Now, load the `choroplethr` package with the `library` command. Then, draw the choropleth map from our data frame `state_1966`. The command is already given, so you just need to press *check*. Note, that we specify a title and the number of colors of the legend. 

```{r "5_6",fig.width=16, fig.height=7}
#< task_notest
library(choroplethr)

state_choropleth(state_1966, title="Panel A. State gasoline taxes (cpg),1966", num_colors=6)
#>

```

In 1966, the variation across states was marginal. The levied state gasoline taxes were concentrated between $5$ and $7.5$ cpg. As you can see from the legend, the higher the tax, the darker the state is shaded. Only Washington, Nebraska and Arizona levied the highest tax of $7.5$ cpg. 

**Task:** Now, we want to perform the exact same task for the year 2006. First, we prepare the data for the choropleth map, then we draw it. For more help, press the *hint* button. 

```{r "5_7",fig.width=16, fig.height=7}
#< task

#>

state_2006 <-adat %>%
  filter(year=="2006") %>%
  select(state,sgastax) %>%
  rename(region=state, value=sgastax)

state_choropleth(state_2006, title="Panel B. State gasoline taxes (cpg), 2006", num_colors=6)

#< hint
display ("You can easily copy the code from the previous two code chunks and then change the year 1966 to 2006 wherever it occurs.")
#>
```

*These graphs correspond to Figure 2 on page 308 of the paper.*

#< quiz "State Tax Higher than Federal Tax"
question: Check all states with a state tax higher than the federal tax in 2006.
mc:
- Pennsylvania*
- North Carolina*
- South Carolina
- Washington*
- Florida
- Oklahoma
success: Great, all answers are correct!
failure: Not all answers are correct. Try again.
#>

#< award "Map Expert"
Congrats, not only did you learn how to draw choropleth maps of the U.S. states, but you can also read them and assign the values to the right states. 
#>

During the course of the sample, the state gasoline taxes grew and became more disperse. In 2006, the state taxes ranged from $7.5$ cpg to $34$ cpg, with a much higher between-state variation. By 2006, $74.5$% of the states had a higher gasoline tax rate than the federal tax of $18.4$ cpg. In about a quarter of the states, the tax was higher than $25$ cpg. Georgia's state tax was the only tax rate under $10$ cpg in 2006. 


## Exercise 2 -- Causal Effects of Gasoline Taxes on Retail Gasoline Prices 


In the previous section, we investigated the development of state and federal taxes across states and over time. Within this section, we examine the composition of the retail gasoline prices in the U.S. We want to find out to which extent the defined gasoline tax is borne by the consumers, the so-called tax incidence. How do gasoline tax increases affect retail gasoline prices? Are the taxes passed on entirely to the consumers or do other institutions such as gas stations pay part of it as well? In other words, we want to calculate the magnitude of the causal effect of gasoline taxes on retail gasoline prices. 

Causal relations can be explained and calculated with the help of **linear regression modeling** and the **ordinary least squares (OLS) estimation**, which are the core models of econometrics (cf. Verbeek (2004), p. 7). The main task of econometrics is to analyze,  quantify and interpret economic relationships, such as individual wages and education. Hence, econometrics can be described as an interaction of economic theory, observed data and statistical methods (cf. Verbeek (2004), p. 1).
The outcomes of econometric estimations can be utilized to evaluate and implement government and business policy and are relevant in theoretically all branches of applied economics (cf. Woolridge (2013), pp. 1-2).


### The Simple Linear Regression Model 


For reasons of comprehensibility, we will start with a simple linear regression model where we study how the gasoline retail price varies with changes in the gasoline taxes. In this case, the tax-inclusive gasoline price for state $s$ and year $t$ is called the dependent or explained variable. Consequently, the gasoline tax is called the independent or explanatory variable. We will estimate the effect of a change in the explanatory variable (here: gasoline tax) on the variable storing the gasoline retail prices (cf. Woolridge (2013), p. 23). We can express this relationship with the following formula:

$$RetailPrice_{st}=\beta_{0} + \beta_{1}\tau_{st} +\epsilon_{st} \tag{1}$$


$\beta_{1}$ is of central interest to the linear regression estimation as it is the slope parameter of the relation of the gasoline retail price and the gasoline tax and quantifies it.
In other words, $\beta_{1}$ shall measure the causal effect of $\tau_{st}$ on the $RetailPrice_{st}$. If $\tau_{st}$ changes the value by $1$, the slope parameter shows how much this affects the $RetailPrice_{st}$ and in which direction the effect runs (cf. Woolridge (2013), p. 23).  Within the linear regression model, we want to estimate the slope parameter as exact as possible. The estimate is usually denoted with a hat: $\hat{\beta_{1}}$. With this estimated value $\hat{\beta_{1}}$, the fitted values of the dependent variable $RetailPrice_{st}$ can be calculated: $\hat{RetailPrice_{st}}$. If the fitted values are subtracted from the actual values ($RetailPrice_{st}$ - $\hat{RetailPrice_{st}}$ ), the so-called residuals are produced. They can be interpreted as estimates of the unknown disturbances in the data set: the error term $\epsilon_{st}$. It represents all unobserved factors other than the sum of federal and state excise taxes $\tau_{st}$ that affect the $RetailPrice_{st}$. 

The objective is that the estimated value $\hat{\beta_{1}}$ is as close to the actual value of $\beta_{1}$ as possible. We aim for a consistent and non-biased estimator. We will learn more about the bias in the course of this exercise. 

**Task**: In order to start with *Exercise 2*, we first need to load our data frame `annual_data.rds`, known from *Exercise 1*, again with the `readRDS()` command and assign it to the variable `adat`. As it is the first code chunk in *Exercise 2*, you need to press *edit* first. 

```{r "6_1"}
#< task

#>
adat <- readRDS("annual_data.rds")

``` 

In preparation for our linear model, we need to drop the data for the years 1966, 1967 and 2009 and 2010, as the data frame is not complete for those years. Values of the nominal oil price are missing. Consequently, we need to exclude those years from our data frame. 

#< quiz "select or filter"
question: Which function do we need in order to exclude the rows with the previously stated years? 
sc:
- filter*
- select

success: Great, your answer is correct!
failure: Try again.
#>


**Task:** Exclude the years before 1968 and after 2008 from our data frame `adat` with the help of the`filter()` command. Assign the resulting data frame to the variable `annual_regoutput`. Then show the data frame with the `head()` command. Part of the code is already given.

```{r "6_2"}
#< fill_in
annual_regoutput <- adat%>%
  ___(year> ___ & year< ___)
___
#>
annual_regoutput <- adat %>%
  filter(year>1967 & year<2009)

head(annual_regoutput)

#< hint
display("Replace the first ___ with the filter command. Note that the function wants the years to less or more, not less or equal or more or equal.")
#>

```

We can see, that the data frame now starts with the year 1968 and the missing values are omitted. We are able to perform our first linear regression with the `lm()` function. As we only have one explanatory variable to start with, the basic structure of the function is: `dependent variable ~ explanatory variable`. 

#< info "lm()"

`lm()` is R's command to fit *linear models*, hence the name. We always need the formula `dependent variable ~ explanatory variable` as an argument. Assume that the values of our dependent variable are stored in the vector `y` and the values of the explanatory variable in the vector `x` accordingly. Then our formula for the calculation of the OLS coefficients would be: **lm(y~x)**, where we want the left-hand-side variable to be explained by the variable on the right-hand-side. When we add more explanatory variables later on, they can be added with a `+` operator.  Often, our variables are given in a data frame. Then, we can easily include the data frame, for example called `dat` by calling: **lm(y~x, data=dat)** (cf. Heiss (2016), p. 70).

If you want more information, call `help(lm)`. 

#>

**Task:** Regress the `taxin_gas_price` on `gas_tax_all` using the function `lm()` and save the result in the variable `reg_2a`. The variables are stored in the data set `annual_regoutput`.

```{r "6_3"}
#< task

#>
reg_2a <- lm(taxin_gas_price ~ gas_tax_all, data=annual_regoutput)
#< hint
display("Take a look at the info box above to see the arguments of the `lm()`command")
#>
```

**Task:** Now show the results of `reg_2a` with the `summary()` function. 

```{r "6_4"}
#< task

#>
summary(reg_2a)

#< hint
display ("Just add reg_2a in the brackets, as the function needs the results of the preceding regression in R as an argument.")
#>

```

#< info "Regression Output of summary()"

After performing a linear regression with the `lm()` command, the function `summary()` shows the output. 

The information of the `summary()` command is divided into four parts: *Call*, *Residuals*, *Coefficients* and *Statistics*. *Call* shows the formula that R used to fit the data - the given `lm()` command. The *Residuals* section displays some summary statistics for the residuals of the fitted model, such as the median.

The part headed *Coefficients* is of most interest to us since is shows the estimates of the regression coefficients. The explanatory variables (and the intercept) are listed in the first column. For each of the explanatory variables, the subsequent four columns show the following statistics:

+ *Estimate* contains the values of the estimators $\hat{\beta}_i$ for each $\beta_i$.

+ *Std. Error* is the **standard error of the coefficient**. That means it is the estimate of the standard deviation of $\hat\beta_i$. Smaller standard errors relative to the estimators are preferred.

+ *t value* is the number of standard errors that $\beta_i$ is away from zero. With the t-statistics the null hypothesis $H_{0}$:$\beta_i$ = $0$ is tested. Large t-values indicate the rejection of the null hypothesis (cf. Woolridge (2013), pp. 121-123).

+ *Pr(>|t|)* is the probability of observing a value equal to or greater than the absolute t-value, if $H_{0}$ is satisfied. These values are called **p-values**. The p-value indicates the smallest level of significance for which the observed test statistical value results in a rejection of the null hypothesis $H_{0}$ (cf. Kennedy (2013), p. 508). Small p-values are evidence against $H_{0}$. Hence, small p-values indicate that there is a real relationship between the dependent and explanatory variable. In econometrics literature, a p-value of $5$% or lower is said to be a good measure for the significance of a variable. R indicates the different p-values with different symbols for each level of significance. The meaning of those is specified below the estimates in the section *Signif. codes*.

Concerning the statistics at the very end of the output we will only focus on the *Multiple R-squared* and its adjusted version. The $R^2$ can be interpreted as the fraction of the variation in the dependent variable that is explained by the explanatory variables. So it is providing a measure of how well the model is fitting the actual data. Its value lies between 0 and 1. For example a $R^2$ of 0.75 means that 75% of the variance of the dependent variable can be explained by the explanatory variable. In regressions with multiple explanatory variables, the $R^2$ is adjusted to the number of variables (cf. Stock and Watson (2007), p. 123). 

#>

Looking at the summary statistics, we focus on the estimate of the coefficient $\beta_{1}$. The interpretation of the intercept $\beta_{0}$ is not relevant in our model and therefore left out. 

The estimate of $\beta_{1}$ is approximately $3.48$ and can be interpreted as follows. If the gasoline tax increases by $1$ cpg, the retail gasoline price increases by $3.48$ cpg. The standard error amounts to $0.08$. Additionally, the R-squared is $0.4868$, that is, $48.68$ % of the variance found in `taxin_gas_price` can be explained by the explanatory variable. In our case we see three stars next to the estimator for `gas_tax_all`, which means the p-value is smaller than $0.001$. The scaling is visible at the bottom of the table. This means that if the true correlation was zero, the probability that we find an estimator that is at least as high as the one we have found is less than $0.1$ percent.

Is it plausible that an increase of the gasoline tax of $1$ cpg leads to a ceteris paribus increase of the retail gas price of $3.48$ cpg? This tax shifting seems extreme. We likely have a problem with endogeneity: it is probable that the regressor $\tau_{st}$ is correlated with the error term $\epsilon_{st}$: $Cor(\tau_{st},\epsilon_{st})0$. Thus, it is called endogenous. Endogeneity leads to biased estimates, which is undesirable for our regression (cf. Kennedy (2013), p. 139). The key assumption for the simple linear regression model is: the expected value of $\epsilon$ given any value of $\tau_{st}$ is zero. This assumption might be hurt here (cf. Woolridge (2013), p. 45). 


Consequently, we need to put some thought into the question which variables are included in the error term that are correlated with the gasoline tax. 


### Omitted Variable Bias


Likely, the gasoline retail price is affected by the crude oil price and it is possible that the crude oil prices are correlated with the gasoline taxation. 

Let us suppose, that the `gasoline taxation` and the `crude oil price` are the only factors influencing the `gasoline retail price`. We assume that they are exogenous variables, thus they are not correlated with the error term $u_{st}$. 

We add the slope parameter $\beta_{2}$ and the $OilPrice_{t}$ to our equation. Note that the crude oil price is determined on the international oil price market and is equal in all states. Consequently, the $OilPrice_{t}$ is only indexed with a $t$ for the respective year. We obtain the following formula: 

$$RetailPrice_{st}= \beta_{0} + \beta_{1} \tau_{st} + \beta_{2}OilPrice_{t} +u_{st} \tag{2}$$

The longer regression formula is equivalent to the first one, if we specify: 

$$ \epsilon_{st}= \beta_{2}OilPrice_{t} +u_{st} \tag{3}$$

**Task:** Let us run the new, longer regression in R as well and compare the results. Just press *check* to see the result.

```{r "6_5"}
#< task_notest
reg_2b <- lm(taxin_gas_price ~ gas_tax_all+nom_oilprice , data=annual_regoutput)
summary(reg_2b)
#>
```

As we can see, the addition of the crude oil price as a control variable decreases the result for the coefficient estimate of the gas tax $\hat\beta_{1}$ from $3.48$ to $1.66$. 

Imagine, we wouldn't have added the crude oil price to our regression model and thought the first simple regression model was the true one. We omitted a variable which influences the retail oil price and correlates with our explanatory variable, the gasoline tax. As Woolridge puts it, we excluded a relevant variable or underspecified the model (cf. Woolridge (2013), p. 88). Then, our estimates would have been biased.

In the short model $(1)$, `gas_tax_all` is correlated with the error term $\epsilon$ as it contains the `nom_oilprice`. When estimating only the short model, we get a biased estimator $\hat{\beta}_1$ with: 

$$
Bias(\tilde{\beta}_1) = \mathbb{E}(\tilde{\beta}_1) - \beta_1 = \beta_2 \cdot \frac{Cov(gas\_tax\_all,nom\_oilprice)}{Var(gas\_tax\_all)} \tag{4}
$$

where the right-hand-side of the estimation shows the **omitted variable bias**.  

#< info "Information about the Derivation of the Bias Formula"

When we compare the short and the long regression, the difference lies in the number of included explanatory variables. Whereas the short regression only includes `gas_tax_all`, in the long regression `nom_oilprice` is also added. Our primary interest lies in $\beta_1$, the partial effect of `gas_tax_all` on the `taxin_gas_price`. In order to get an unbiased estimator, we should run a regression of `taxin_gas_price` on `gas_tax_all` and `nom_oil_price`. Instead, assume that due to ignorance or data unavailability, we run the short regression $(1)$ without `nom_oilprice`. We can rewrite the short regression as:

$$\tilde{RetailPrice_{st}}=\beta_{0} +\beta_{1}\tau_{st} +\epsilon_{st}$$

We want to derive the bias in the simple regression estimator of $\beta_{1}$.
Wooldridge (2013, p.79, 89) provides a formula for the relationship between the coefficients of the shorter and longer regression: 
$$\tilde{\beta_{1}}=\hat{\beta_{1}}+\hat{\beta_{2}}\tilde{\delta_{1}}$$ 
Note, that $\tilde{\beta}_0$ and $\tilde{\beta}_1$ are the estimators of the short regression$(1)$ and $\hat{\beta}_0$, $\hat{\beta}_1$ and $\hat{\beta}_2$ denote the estimators of the long regression $(2)$. The "~" emphasizes that $\tilde{\beta_{1}}$ comes from an underspecified model. 

$\tilde{\delta_{1}}$ is the slope parameter $\delta_{1}$ of the simple regression of `nom_oilprice` on `gas_tax_all`: 

$$OilPrice_{t}=\alpha+\tilde{\delta_{1}}\tau_{st}+v_{st}.$$

In order to verify that an estimator is unbiased it has to be shown that the expected value of the estimator is equal to the true estimator. In other words, the difference between the expected value of the estimator and the true estimator is zero. Thus, we check if $$\mathbb{E}(\tilde{\beta_{1}})-\beta_{1}= 0$$ 

Calculating $\mathbb{E}(\tilde{\beta_{1}})$ we can use the properties of expected values and obtain that 
$$\mathbb{E}(\tilde{\beta_{1}})=\mathbb{E}(\hat{\beta_{1}}+\tilde{\delta_{1}}*\hat{\beta_{2}}) =\mathbb{E}(\hat{\beta_{1}})+ \tilde{\delta_{1}}*\mathbb{E}(\hat{\beta_{2}})=\beta_{1}+\tilde{\delta_{1}}*\beta_{2}$$ which is in general not equal to $\beta_{1}$. The bias can be computed by the difference of the expected value and the true estimate, which can be written as $$bias(\tilde{\beta_{1}})=\beta_{1}+\tilde{\delta_{1}}\beta_{2}-\beta_{1}=\tilde{\delta_{1}}\beta_{2}$$ (cf. Wooldridge (2013), pp.88-91). 

$\tilde{\delta_{1}}$ is the sample covariance between `gas_tax_all` and `nom_oilprice` over the sample variance of `gas_tax_all`: $\tilde{\delta_{1}}=\frac{Cov(gas\_tax\_all, nom\_oilprice)}{Var(gas\_tax\_all)}$.

$\tilde{\delta_{1}}$ = 0, if `gas_tax_all` and `nom_oilprice` are uncorrelated in the sample. Leading to $\tilde{\beta_{1}}$ being unbiased if `gas_tax_all` and `nom_oilprice` are uncorrelated. 

If they are correlated, however, we can calculate the bias with the following formula: 


$$
Bias(\tilde{\beta}_1) = \mathbb{E}(\tilde{\beta}_1) - \beta_1 = \beta_2 \cdot \frac{Cov(gas\_tax\_all,nom\_oilprice)}{Var(gas\_tax\_all)}
$$
#>

#< quiz "omitted variable bias"
question: What would happen if the omitted variable `crude oil price` influenced the retail oil price ($\beta_20$) but is not correlated with the gasoline tax? 

sc:
- Beta 1 would still be biased.
- Beta 1 would not be biased.*

success: Great, you answered correctly!
failure: Try again.

#>

To sum it up, if $\tilde{\delta_1}= 0$, $\tilde{\beta_1}$ is unbiased for $\beta_1$, even if $\beta_2$ is not zero. There is no bias if the explanatory variables are not correlated in the sample.

The other case, where $\tilde{\beta_1}$ is not biased, is if $\beta_{2}=0$. Then the additional explanatory variable of the long regression does not appear in the true model, because it has no influence on the dependent variable (cf. Woolridge (2013), p. 90). 

We now want to use the **omitted variable bias** formula $(4)$, to calculate the bias in our example $Bias(\tilde{\beta}_1)$. 

**Task**: Just press *check* in order to calculate the bias. 

```{r "6_6"}
#< task_notest

# Determine x1 and x2.
x1 = annual_regoutput$gas_tax_all
x2 = annual_regoutput$nom_oilprice
# Take the beta_2 from the long regression.
beta2_hat = coef(reg_2b)[3]
# Calculate the bias_hat.
bias_hat = beta2_hat * cov(x1, x2) / var(x1)
# Show the bias_hat.
bias_hat
#>
```

In the above code chunk we calculated the bias by dividing the covariance by the variance and found a $\hat{Bias(\tilde{\beta}_1)}$ of $1.82$. This means, we overestimated the positive effect of the gasoline taxes on the retail gasoline price. The difference between the absolute value of $\tilde{\beta_1}$ and the true value $\hat{\beta_1}$ is the bias we computed: 

$$ \hat{Bias(\tilde{\beta}_1)}= \tilde{\beta_1}-\hat{\beta_1}$$

This can be verified with the coefficient results of our regressions $(1)$ and $(2)$. Note that with the regressions, the bias is only estimated. 

**Task**: Just press *check* to save the coefficients of our two estimations in the variables `beta1_tilde` and `beta1_hat`.

```{r "6_7"}
#< task_notest
# Save the beta1_tilde from the short regression.
beta1_tilde = coef(reg_2a)[2]
# Save the beta1_hat from the long regression.
beta1_hat = coef(reg_2b)[2]

#>
```

**Task**: Now calculate the bias from the estimated regression coefficients by using the code chunk as a calculator. Just subtract `beta1_hat` from `beta1_tilde` to get the right hands side of the above formula. 

```{r "6_8"}
#< task

#>

beta1_tilde - beta1_hat


```

We also find here that the estimated bias we had in the short regression without considering the `nominal oil price` was approximately $1.82$. By that, we have illustrated that our short regression model has been biased. 


### Multiple Regression Model


So far, we assumed that the long model with two explanatory variables `gas_tax_all` and `nom_oilprice` is our true model, supposing the error term is indeed uncorrelated with the explanatory variables. Differently put, we assume that no other confounders or confounding variables are part of the error term. Confounders are variables that fulfill the following conditions: they are causally related with the dependent variable and they are positively or negatively correlated with the explanatory variable of interest. Thereby, they change the estimate of the coefficient estimation (cf. McNamee, 2003). We thus need to check whether there are other relevant omitted variables that influence the outcome of our regression estimation. 

In our case, there are most likely additional factors, which influence the retail oil price and are correlated with the gasoline tax at the same time. In the next task, we will add more variables and extent our multiple regression. 

From our own experience, we know that the retail gasoline price does not only change between states but also from gas station to gas station. Independent from the state and federal gas taxes, prices can differ in multiple locations due to other circumstances such as the rent of the gas stations or transportation costs. For instance, renting the space for a gas station in New York City will be much more expensive than in a small rural town in Pennsylvania. As this can also correlate with the explanatory variable $\tau_{st}$, the states are also a confounder for which we control with state fixed effects $a_{s}$. 

The general omitted variable bias problem remains, though. Other confounders can exist even if we add a number of control variables. 

Possibly, there is a time trend in action that influences both the retail gasoline price and the gasoline taxes. Economic data is generally prone to time trends. You can learn more about time trends in the *info box* below. Thus, Li et al. add a linear time trend to the equation to account for time dependent factors that both influences the retail price and the taxes. 


Mathematically we can express this relationship with the following multiple regression formula: 


$$RetailPrice_{st}= \beta_{0}+\beta_{1} \tau_{st}+ \beta_{2} OilPrice_{t}+a_{s}+\delta t+\epsilon_{st}$$

We regress the tax-inclusive price per state and year $RetailPrice_{st}$ on federal and state excise taxes $\tau_{st}$, the crude oil price $OilPrice_{t}$, state fixed effects $a_{s}$ and linear time trends $\delta_{s}$. All data sets used within this problem set are panel data sets. Before learning about fixed effects and time trends, it is important to know the basics about panel data. You can find this information in the *info box* below. 

#< info "Panel Data"

Panel data or longitudinal data combines time series data with cross-sectional units in a data set. Cross-sectional units can be individuals, households, counties, states etc. that are followed over a specific period (cf. Woolridge (2013), p. 10). 

Panel data requires that the same units are observed over time. The higher effort for this is justified with some advantages panel data has compared to pooled cross-sectional data or other forms of data sets. The first advantage is that having multiple observations on the same units helps to control for unobserved characteristics of the units. Moreover, panel data allows to investigate the importance of lags in behavior or the results of decision-making (cf. Woolridge (2013), p. 11).

It is important to note that time-series data, as we have it in panel data sets, requires attention for the correlation across time of most economic time series (cf. Woolridge (2013), p. 16).

#>

#< info "Time Trends"

Economic observations, such as our sample data, are very rarely, if ever, independent across time. Variables of economic data, which are usually gathered as a time series, are often strongly related to their recent histories. Past events can possibly have an influence on future values. Therefore, time is an important dimension in time-series data sets and the chronological order of observations contains potentially important information. Moreover, economic time series are of trending, highly persistent nature. Often, they tend to grow over time (cf. Woolridge (2013), pp. 8-11, 363-364).

#>

#< info "Fixed Effects"

As stated above, within panel data sets, the same units are observed over time and those observations are very rarely independent over time. Consequently, there are unobserved factors, which influence the observations of a certain unit over the years. We add $a_{s}$ to our regression model, which captures all unobserved, time-constant factors that affect the dependent variable $RetailPrice_{st}$. As $a_{s}$ does not change over time, it has no subscript $t$. $a_{s}$ can be called un unobserved effect and is also often referred to as a fixed effect. Thus, the corresponding model is also called an unobserved effects or fixed effects model. The error term $\epsilon_{st}$ is then referred to as the time-varying or idiosyncratic error, as it subsumes all unobserved variables that affect $y_{st}$ and do change over time. In our example, ${s}$ denotes different states. That's why $a_{s}$ is called a state fixed effect. Geographical features, such as the state's location within the U.S., but also its population's demographics or the urbanization of the state, which are factors that only change over a long period or not at all, are captured by $a_{s}$. Hence, fixed effects control for state-specific characteristics. The state fixed effect is the same within a state, but differs between states (cf. Woolridge (2013), pp. 459-460).

Mathematically, the fixed effects transformation with the aim of removing the fixed effect $a_{s}$ works like this: 

We consider a model with only one explanatory variable: 

$$y_{st}= \beta_1 x_{st}+a_{s}+u_{st}$$ with $t= 1,2,....T$. 

As the next step, we average the previous equation over time, for every state ${s}$. 

$$\bar{y_s}= \beta_1 \bar{x_s}+a_{s}+\bar{u_s}$$

As $a_{s}$ is fixed over time, it is part of both formulas. When subtracting the second equation from the first one for every $t$, we receive: 

$$y_{st}-\bar{y_s}= \beta_1(x_{st}-\bar{x_s})+u_{st}-\bar{u_s} ,    t= 1,2,....T$$

$y_{st}-\bar{y_s}$ is the time-demeaned data on $y$ (similarly for $x$ and $u$). The unobserved factor $a_{s}$ is not part of the equation any more. We can estimate the last formula with pooled OLS, which is then called a **fixed effect estimator** (cf. Woolridge (2013), pp. 484-485). 

In a fixed effects regression model, each entity has its own intercept. By that, influences from omitted variables, which differ between entities but are constant across time are absorbed. These intercepts can be represented by a set of binary variables (cf. Stock and Watson (2007), p. 356).

#>

In order to introduce fixed effects into our regression model, we can add the state fixed effects by using the `as.character()` or `factor()` function. Another way of introducing fixed effects into a linear regression model will be shown in *Exercise 3*. Additionally, the `year` is added as a control variable in order to control for the time trends.

**Task:** Just press *check* to see the result.

```{r "6_9"}
#< task_notest
#Run the regression using lm
reg_2c <- lm(taxin_gas_price ~ gas_tax_all+nom_oilprice +year+factor(fips),
             data=annual_regoutput)

#Output
summary(reg_2c)

#>
```

As you can see, when adding the state fixed effects, the regression output of the `summary()` function gets quite long and hard to read. The reason for that is that the fixed effects estimator adds an individual intercept for the states. On top of that, it would be great to be able to look at all results of our performed regressions at the same time in order to compare them directly. In R, there is a very good package to do so, called `stargazer`. 


#< info "stargazer()"
The R-package `stargazer()` allows us to create well-formatted regression tables with multiple models side-by-side. On top of that, it is possible to easily create summary statistical tables, data frames, vectors and matrices. The benefits of the `stargazer()` package are that it is easy to use, includes a large number of models and has nice aesthetics. 

Several arguments allow us to easily adapt the output of the function - here: our regression table.

- the first argument are the names of the regression models that are ought to be shown
- *type* is a character vector indicating the type of output the table should have, for example *html*, *text* and *latex*.
- With *keep* you can determine which explanatory variables should be shown. If this argument is omitted, all explanatory variables and the intercept are displayed. Alternatively, by means of the argument *omit* the specified variables are omitted.
- With the option *column.labels* you can name the columns in the regression table.
- *covariate.labels* allows to name the displayed explanatory variables. 
- *dep.var.labels* names the displayed dependent variables. 

These are only a few of the many adaption options, but the ones we are gonna use most often during the course of this problem set (cf. Hlavac, 2018).

For more info call `help(stargazer)`.
#>

**Task:** First, we need to load the package with the help of the `library()` function. Just type library and add `stargazer` in the brackets.

```{r "6_10"}
#< task

#>
library(stargazer)
```

**Task:** Just press *check* to see the table of our regressions `reg_2a`, `reg_2b` and `reg_2c``. 

```{r "6_11",results='asis'}
#< task_notest
stargazer(reg_2a, reg_2b, reg_2c, 
          title="Estimation Results from Retail Gasoline Price Regression", 
          keep = c("gas_tax_all", "nom_oilprice", "year"),
          type="html")
#>
```

Looking at the last regression, with time trends and state fixed effects, we see that the coefficient estimate of the gasoline tax lies at $0.958$ in the third column. If the gasoline tax rises by $1$ cpg, there is a $0.96$ cpg increase in the retail price as well. With a standard deviation of $0.046$, this value is statistically indistinguishable from $1$, when defining confidence intervals between plus and minus two standard errors. 

**This result suggests that gasoline taxes are borne entirely by the consumers** (cf. Li et al. (2014), p. 306).

A tax incidence close to $1$ was also found by Marion and Muehlegger. They find that state gasoline and diesel fuel taxes are on average fully and immediately passed on to consumers. Moreover, this effect is not influenced by season nor capacity utilization (cf. Marion and Muehlegger (2011), pp. 2-3). 


## Exercise 3 -- Effects of Gasoline Taxes on Consumer Behavior: Aggregate State Level Analysis


With the newly acquired knowledge about the composition of U.S. retail gasoline prices, we may now dive deeper into the topic and examine the effects the gasoline price components have on consumer behavior. 

Within this section, this problem set is based on the aggregate state-level data including gasoline consumption, taxes and prices at the state level. The purpose of the aggregate data is to estimate the gasoline demand response to tax and price changes and to examine whether there are different responses to changes of those two components (cf. Li et al. (2014), p. 311). 

**Task:** To start again, press *edit* and *check* to load the data set and store it in a variable called `adat_log`. 

```{r "7_1"}
#< task_notest
adat_log <- readRDS("adat_log.rds")
#>
```

**Task:** Take a look at the new columns of our data set by using the `head()` command on our variable `adat_log`.

```{r "7_2"}
#< task

#>

head(adat_log)

#< hint
display("Call head() and add the variable name in the brackets.")
#>
```

The analysis is based on the same data sources as *Exercise 1 and 2*, but more columns of the original data set of the authors are included. Now, demographic and political variables, which describe the states are part of the data set as well, such as the number of licensed drivers, the average family size, the size of the adult population, the GSP per capita, etc. Additionally, the data frame contains new columns with the calculated natural logarithms of the following values:  

- logarithm of the gasoline consumption per adult `lngca`: ln(`hug`/`apop_adj`) 
 + with `hug`: highway gasoline consumption in thousands of gallons
 + with `apop_adj`: adult population from census per state and year, adjusted
- logarithm of the tax-inclusive gas price `lngpinc`: ln(`taxin_gas_price`)
- logarithm of the tax ratio `lntr`: ln(1+(`fgastax`+`sgastax`)/`exclgp`)
 + with `exclgp`: tax-exclusive gas price `taxin_gas_price`- `fgastax`- `sgastax`
 + with `fgastax`: federal gas tax
 + with `sgastax`: state gas tax 
- logarithm of the tax-exclusive gas price `lngp`: ln(`exclgp`)


We start with the **tax-inclusive gas price** as the explanatory variable. The separation of the tax-inclusive gas price into a tax-exclusive part and the tax component is implemented subsequently. First, we regress the original values without logarithms, which is called a level-level model. This functional form was also used in *Exercise 2*. Assume, we have a dependent variable $y$ and an explanatory variable $x$. Then the interpretation of the slope parameter $\beta_{1}$ is the following: $\Delta y=\beta_{1}\Delta x$. If the explanatory variable $x$ changes by $1$, the dependent variable $y$ changes by the value of the slope parameter $\beta_{1}$ (cf. Woolridge (2013), p. 44). Mathematically, we express our linear regression model with the following formula: 

$$q_{sy}=\beta_{0}+\beta_{1}P_{sy}+X_{sy}\theta+\delta_{s}+\phi_{y}+e_{sy}$$

- $q_{sy}$ is the dependent variable: gasoline consumption per adult by state ${s}$ and year ${y}$

- $P_{sy}$ is the tax-inclusive gasoline price
 
- $X_{sy}$ is a vector of state-level covariates and also contains state specific quadratic trends

- $\delta_{s}$ are state fixed effects

- $\phi_{y}$ are year fixed effects

- $e_{sy}$ is the error term

In order to avoid endogeneity problems, the shown specification of the linear regression  model contains state fixed effects, year fixed effects and state specific quadratic trends. Those quadratic trends are aimed at helping us to identify the effect of actual tax changes instead of long-term trends. They are ought to control for variables, which vary gradually over time such as the population of a state, long-run economic growth or land development. All of the before-mentioned are drivers of gaps between the infrastructure needs and transportation funding, which might trigger tax increases (cf. Li et al. (2014), p. 316). 

In *Exercise 2*, we controlled for state fixed effects. Now, time fixed effects are also added. Opposite to state fixed effects, time fixed effects control for variables that change over time but are constant across entities. Then, each time period has an individual intercept (cf. Stock and Watson (2007), pp. 361-362). In our case, the year has an influence on the gasoline retail price and tax, for example by a common trend. 

As mentioned before, there is another way of introducing fixed effects into a linear regression model in R. Other than as a `factor` or `as.character` within the `lm` command, we can include the fixed effects with the help of the `felm` command of the `lfe` package. 

#< info "felm()"

With the function `felm()` of the `lfe()` package, you can easily introduce fixed effects into the regression formula. 

The `felm()` function is used to fit linear models with multiple group fixed effects. As with `lm()`, the `summary()` function can be used to print the results of the regression estimation. The first part of the formula is, similarly to `lm()`, the dependent variable. After the tilde `~`, the dependent variable is followed by one or more explanatory variables. The next part consists of factors to be projected out - the fixed effects, followed by an IV-specification and a cluster-specification for the standard errors. All parts, except for the dependent and explanatory variables, can be left out by adding a $0$ instead of the terms, if we do not need them. So if we only want to introduce fixed effects to our formula, it can look like this: 

```{r "7_3",optional=TRUE, eval=FALSE}

felm(y ~ x1 + x2 | fixed_effects | 0 | 0, data=dat)
```

With the `+` operator, any number of fixed effects can be added. 

You can find more details in the [R Documentation of the lfe package](https://www.rdocumentation.org/packages/lfe/versions/2.8-6/topics/felm).

#>

The specification additionally includes a set of demographic variables, the mentioned state-level covariates $X_{sy}$. According to the authors, those are commonly used in the literature (cf. Li et al. (2014), p. 314):

- `fsize`: the family size
- `lnrma`: the logarithm of the road mileage per adult 
- `lnincpop`: the logarithm of the real income per adult in the specific state 
- `lncarscap`: the logarithm of the number of registered cars per adult 
- `lntrkscap`: the logarithm of the number of registered trucks per adult 
- `lndriverscap`: the logarithm of the number of licensed drivers per adult 
- `urbanization`: the fraction of the population that lives in a metropolitan area 
- `railpop`: the fraction of the population living in metro areas with rail transit 


**Task:** First, load the `lfe` package with the `library` command. Then, use the `felm()` function and assign it to the variable `reg_3a`. Regress the gasoline consumption per adult by state and year `gca` on the tax-inclusive gas price `taxin_gas_price` and add state and year fixed effects, the before mentioned control variables and the state-specific trends. The data frame we use is the `adat_log`. Then show the result with `stargazer`. Just fill in the missing info.

```{r "7_4",results='asis'}
#< fill_in
#Load the package
___
#Run the regression using `felm`
___ <- ___(gca ~ taxin_gas_price+ fsize+ lnrma+ lnincpop +lncarscap +lntrkscap+
           lndriverscap+ urbanization +railpop +state*trend + state*trend2|year+state
           | 0 | 0, data= adat_log)
#Output
___(reg_3a, keep=c("taxin_gas_price"), type="html")
#>
library(lfe)
reg_3a <- felm(gca ~ taxin_gas_price+ fsize+ lnrma+ lnincpop +lncarscap +lntrkscap+
               lndriverscap+ urbanization +railpop +state*trend +
               state*trend2|year+state | 0 | 0, data= adat_log)

stargazer(reg_3a, keep=c("taxin_gas_price"), type="html")

#< hint
display("Call library and add the package name in the brackets. for the regression first assign the variable name reg_3a, then use the felm command. To show the results use stargazer.")
#>
```

In a level-level model, we need to be careful when interpreting the coefficient estimations, because different measurement units have to be taken into account. The unit of the gasoline consumption per adult is thousands of gallons whereas the tax-inclusive gas price is indicated in cents per gallon (cpg). The coefficient estimate $\hat\beta_{1}$ of $-0.001$ means that at an increase of the tax-inclusive gas price by $1$ cent per gallon, the gasoline demand decreases by $1$ gallon per adult. Note, that the standard error is at $0.1$ per gallon. With a p-value smaller than $0.01$, indicated by the three stars next to the coefficient estimate, the estimate is statistically significant at the $1$% level. 

### Clustered Standard Errors


When working with panel data, it is possible that the standard errors within groups, such as the states, are correlated but there is no correlation of the standard errors between the groups. Thus, clustered standard errors are used to obtain fully robust standard errors (cf. Woolridge (2013), p. 483).

#< info "Clustering Standard Errors Using felm()"
With the help of the `felm` command it is easy to introduce clustered standard errors. Remember our example in the first `felm` info box: 

```{r "7_5",optional=TRUE, eval=FALSE}
felm(y ~ x1 + x2 | fixed_effects | 0 | cluster_variables, data=dat)
```

Here, the last part can be used for clustering the standard errors. In our case, we cluster the standard errors per `state`. Note, that we add a $0$ for the IV-specification again as we do not need it here. 

#>

**Task**: Just press *check* to solve the next code chunk. We compare the standard errors afterwards. 


```{r "7_6",results='asis'}
#< task_notest

reg_3b <- felm(gca ~ taxin_gas_price+ fsize+ lnrma+ lnincpop +lncarscap +lntrkscap+
               lndriverscap+ urbanization +railpop +state*trend +
               state*trend2|year+state | 0 | state, data= adat_log)

stargazer(reg_3a,reg_3b, type="html", keep=c("taxin_gas_price"))
#>
```

As you can see, the addition of the clustered standard error did not change the coefficient estimate of the `taxin_gas_price`. However, the standard error increased from $0.1$ to $0.2$ per gallon. 

With different measurement units, one has to be extra careful when interpreting the estimation coefficients.That's why, we now use the logarithm of the dependent and explanatory variables.  By that, the price and tax elasticizes of demand can be derived from the coefficients of the equation. 

#< info "Logarithm and Elasticity in Linear Regression Models"

When using logarithms in linear regressions, the coefficients have percentage change interpretations. Thus, the measurement units do not have to be taken into account. Dependent as well as explanatory variables can be logarithmic. With a $log(y)$ for a dependent variable, often the classical linear model assumptions are better satisfied. Additionally, using the log can reduce the variation of a variable in many cases. Then the OLS estimates are less prone to external influence (cf. Woolridge (2013), p. 216).

On top of that, the slope  parameter $\beta$ of the logarithmic variables can be interpreted as an elasticity (cf. von Auer (2016), p. 194). If both, the dependent variable $y$ and the explanatory variable $x$ are logarithms, $\beta$ shows the elasticity of the dependent variable $y$. Hence, it provides the percentage change of $y$ when the explanatory variable $x$ rises by $1$% (cf. von Auer (2016), p. 339).

#>

We will use both, the logarithm of our dependent variable gasoline consumption and the logarithm of our explanatory variable tax-inclusive gas price. 

**Task:** Use the `felm()` function and assign it to the variable `reg_3c`. Regress the logarithm of gasoline consumption per adult by state and year `lngca` on the logarithm of the tax-inclusive gas price `lngpinc`. As before, add the set of demographic control variables, the state-quadratic time trend and the year and state fixed effects. Use state clustered fixed effects. The data frame we use is the `adat_log`. 

```{r "7_7"}
#< fill_in
___ <- ___(lngca ~ lngpinc+ fsize+ lnrma+ lnincpop +lncarscap +lntrkscap+
           lndriverscap+ urbanization +railpop +state*trend + state*trend2|year+state
           | 0 | ___, data= adat_log)

#>

reg_3c <- felm(lngca ~ lngpinc+ fsize+ lnrma+ lnincpop +lncarscap +lntrkscap+
               lndriverscap+ urbanization +railpop +state*trend +
               state*trend2|year+state | 0 | state, data= adat_log)

#< hint
display("First, assign the regression model to the variable reg_3c. Then use the felm() command. Furthermore, add state at the spot for the state clustered fixed effects.")
#>
```

Let's look at the results of the last regression with the `stargazer` command in order to focus on the demand elasticity of the gasoline consumption with changes of the tax-inclusive gasoline price. 

**Task:** Just press *check* to see the result. 

```{r "7_8",results='asis'}
#< task_notest
stargazer(reg_3c,
          title="Estimation Results from Gasoline Consumption Regression",
          type="html",
          keep=c("lngpinc"))
#>
```

The performed regression 3d provides an estimate of $-0.097$ for the price elasticity of gasoline consumption with a standard error of $0.036$. When the tax-inclusive gasoline price rises by $1$%, the gasoline demand decreases by approximately  $0.1$%. According to the authors, this aligns broadly with previous results in the literature (cf. Li et al. (2014), p. 314). 

Now, we want to get closer to our main question. Therefore, we separate the tax-inclusive price into a tax component and a tax-exclusive component. The state and year fixed effects, state-quadratic time trends and demographic control variables remain the same. 

As we recall, our findings in *Exercise 2* have shown strong evidence that gasoline taxes are passed on fully to the consumers. Assuming that consumers bear the entire tax, the tax-exclusive price is not affected by a change in the tax rate. Consequently, $dp/d\tau$ is equal to zero. Following this argumentation, we can test directly, whether taxes are more strongly correlated with behavior than tax-exclusive retail prices. If consumers respond equally to the changes in the gasoline tax and tax-exclusive price of the same size, $\beta_{1}$ should be equal to $\beta_{2}$. In the subsequent analysis, this equality will be tested and consecutively, the effects of the gasoline tax and tax-exclusive prices can be compared. However, if consumers respond more to a tax change than to a change in the tax-exclusive price, we would get:

$$(|\frac{\delta(q)}{\delta\tau}| > |\frac{ \delta(q)}{\delta p}|)$$.

$\beta_{1}$ should be larger than $\beta_{2}$ in magnitude. 

We want to estimate this linear regression model: 

$$q_{sy}=\beta_{0}+\beta_{1}\tau_{sy}+\beta_{2}p_{sy}+X_{sy}\theta+\delta_{s}+\phi_{s}+e_{sy}$$

- $q_{sy}$ is the dependent variable: gasoline consumption per adult by state ${s}$ and year ${y}$

- $\tau_{sy}$ is the total state and federal gasoline tax

- $p_{sy}$ is the tax-exclusive gasoline price

With the help of this formula, we can examine whether higher taxes have a bigger impact on the gasoline consumption of consumers than higher tax-exclusive gasoline prices of the same magnitude. 


#< quiz "main question"
question: How will consumers respond, if hypothetically the retail gasoline price rises through increases of the gasoline tax and tax-exclusive gasoline price by the same amount?
sc:
- Consumers react equal to both increases.
- Consumers react more strongly to the increase of the tax-exclusive gasoline price.
- Consumers react more strongly to the increase of the gasoline tax.*
#>

**Task**: Regress the gasoline consumption `gca` on the total state and gasoline tax `gas_tax_all` and the tax-exclusive gasoline price `exclgp` and assign it to the variable `reg_3d`. Then show the results of the regression with stargazer. Part of the code is already given.

```{r "7_9",results='asis'}
#< fill_in
___ <- ___(gca ~ gas_tax_all+exclgp+ fsize+ lnrma+ lnincpop +lncarscap +lntrkscap+
           lndriverscap+ urbanization +railpop +state*trend + state*trend2|year+state
           | 0 | state, data= adat_log)

___(___, type="html", keep=c("gas_tax_all", "exclgp"))
#>
reg_3d <- felm(gca ~ gas_tax_all+exclgp+ fsize+ lnrma+ lnincpop +lncarscap
               +lntrkscap+ lndriverscap+ urbanization +railpop +state*trend +
               state*trend2|year+state | 0 | state, data= adat_log)

stargazer(reg_3d, type="html", keep=c("gas_tax_all", "exclgp"))

#< hint
display("First, assign the regression model to the variable reg_3d. Then use the felm() command. For the output, use stargazer and also add the variable name in the brackets.")
#>
```

As before, the measurement units need to be taken into account for the interpretation. An increase of the tax-exclusive retail price by $1$ cpg, leads to a decrease of the gasoline demand of $0.3$ gallons per adult and year. The increase of the gasoline tax by $1$ cpg, however, leads to a decrease of $2.0$ gallons per adult per year. 

Now, let us perform the regression with logarithmic values again. 
As mentioned above, the log of the gasoline demand and tax-exclusive gas price is already included in the data frame. On top of that, we need the logarithm of the sum of the federal and state gas tax `gas_tax_all`. This can easily be done with `mutate` and the `log` function, which calculates the natural logarithm by default. 

**Task**: Add a new column called `lntax` of the log of the `gas_tax_all` to our data frame `adat_log` with the help of the `mutate()` command. 

```{r "7_10"}
#< fill_in
dat_log <- adat_log %>%
  ___(lntax=___))

#>
dat_log <- adat_log %>%
  mutate(lntax=log(gas_tax_all))

#< hint
display("Use mutate to generate the new column. lntax is simply calculated as the log of the gas_tax_all.")
#>
```

**Task:** Use the `felm()` function for the linear regression model and assign it to the variable `reg_log`. Again, the dependent variable is the logarithm of the gasoline consumption per adult `lngca`. It is regressed on the logarithm of the federal and state tax `lntax` and the logarithm of the tax-exclusive gas price `lngp`. Other control variables, state-quadratic time trends and year and state fixed effects are added. The standard errors are again clustered by state. The data frame we use is the `reg_log`. Show the results with `stargazer`.
Again, part of the code is already given. 

```{r "7_11",results='asis'}
#< fill_in
___ <- ___(lngca ~ ___+___+ fsize+ lnrma+ lnincpop +lncarscap +lntrkscap+
           lndriverscap+ urbanization +railpop+state*trend+state*trend2|year+state |
           0 | state, data= dat_log)

stargazer(reg_log, type= "html", keep = c("lntax", "lngp"))
#>
reg_log <- felm(lngca ~ lntax+lngp+ fsize+ lnrma+ lnincpop +lncarscap +lntrkscap+
                lndriverscap+ urbanization+ railpop+ state*trend+ state*trend2
                |year+state | 0 | state, data= dat_log)

stargazer(reg_log, type="html" ,keep = c("lntax", "lngp"))

#< hint
display("Assign the function to the specified variable. Then add the explanatory variables that are mentioned in the text.")
#>
```

The coefficient of the tax-exclusive gasoline price `lngp` is  $0.043$ with a standard error of $0.027$ whereas the coefficient of the tax rate `lntax` is  $0.098$ with a standard error of $0.017$. The result of the tax rate coefficient is significant at the  $1$ % level, whereas the effect of the tax-exclusive gas price is not statistically significant. Nevertheless, it is visible, that again, the effect of the tax component is more than double as high as the effect of the tax-exclusive component. The coefficient of the tax component can be interpreted as follows: If the tax rate rises by $1$%, the gasoline demand will decrease by $0.1$%. 

Instead of the absolute value or the logarithm of the state and federal taxes, Li et al. use the **logarithm of the tax ratio** in their linear regression model as an explanatory variable for the tax component:
$$(1+\frac{\tau_{sy}}{p_{sy}})$$
By that, they get the mean consumer response to an increase in gasoline taxes divided by an equivalent increase in market prices (cf. Rivers and Schaufele (2012), p.2). However, the authors do not further explain the reasons behind the use of the tax ratio instead of the tax rate. 

We want to perform  a similar log-log regression as before, where the dependent variable is the logarithm of the gasoline consumption per adult and state and one explanatory variable is the logarithm of the tax-exclusive retail price `lngp`. This time, however, the explanatory variable of the tax component will be the tax-ratio `lntr` as explained above. 

**Task:** Use the `felm()` function for the linear regression model and assign it to the variable `reg_3e`. Regress the logarithm of the gasoline consumption per adult `lngca`on the logarithm of the tax ratio `lntr` and the logarithm of the tax-exclusive gas price `lngp`. Other control variables, state-quadratic time trends and year and state fixed effects are added. The standard errors are again clustered by state. The data frame we use is the `dat_log`. Press *check* to run the regression. 

```{r "7_12"}
#< task_notest
reg_3e <- felm(lngca ~ lntr+lngp+ fsize+ lnrma+ lnincpop +lncarscap +lntrkscap+
               lndriverscap+ urbanization+railpop+state*trend+state*trend2|year+state
               | 0 | state, data= dat_log)
#>
```

**Task:** Let us compare the last regression where the gasoline retail price is split into the tax and tax-exclusive component and the log regression with the tax-inclusive price as explanatory variable. Use the `stargazer()` command to show our regressions `reg_3c` and `reg_3e` in one table.

```{r "7_13",results='asis'}
#< fill_in

stargazer(___, ___,  
          title="Estimation Results from Gasoline Consumption Regression", 
          type="html",
          keep=c("lngpinc", "lntr", "lngp"),
          omit = c("lntrkscap"),
          covariate.labels = c("log(gas price)", 
          "log(1+tax ratio)", 
          "log(tax-excl.price)"))
#>
stargazer(reg_3c,reg_3e,  
          title="Estimation Results from Gasoline Consumption Regression", 
          type="html",
          keep=c( "lngpinc", "lntr", "lngp"),
          omit = c("lntrkscap"),
          covariate.labels = c("log(gas price)", 
          "log(1+tax ratio)", 
          "log(tax-excl. price)"))

#< hint
display("Fill in the names of the regression models in the blank spaces.")
#>
```

#< award "Stargazing at its finest!"
Great, you repeatedly used `stargazer` to show the regression results and know how to work with that useful function. 
#>


Again, the coefficients of our explanatory variables ar of most interest to us. Looking at our last regression, the coefficient estimate of the tax-exclusive gasoline price `lngp` is $-0.113$ whereas the coefficient of the tax ratio `lntr` is $-0.292$. Now, both coefficient estimates are significant at the $1$% level. It is striking, that the coefficient estimate of the tax component is much larger than the one on the tax-exclusive price. With different specifications the authors tested the hypothesis whether the two coefficient estimates are equal. They found that $\beta_{1}$ and $\beta_{2}$ are not equal at the $1$% significance level. Moreover, the effect of the tax component is larger than that of the tax-inclusive retail price (cf. Li et al. (2014), p. 314).

**The results show that consumers react more strongly to gasoline tax changes than equivalent changes in tax-exclusive price components or the tax-inclusive retail price.**

Generally, these results for the gasoline demand response to changes in price and tax are consistent with other relevant sources with reference to gasoline demand elasticity. 
Rivers and Schaufele (2012) explored the effects of British Columbia's carbon tax on fossil fuels. According to the authors, the short-run gasoline demand declined significantly greater because of the carbon tax than what would be expected from a commensurate increase in the market price of gasoline. The demand response to the carbon tax was found to be 4.9 times larger than that of an equivalent change in the carbon tax-exclusive price (cf. Rivers and Schaufele (2012), p. 2).
Tiezzi and Verde (2016) also found greater demand elasticity concerning the gasoline tax compared to market-induced price changes by seven times. Interestingly, the authors also evaluated different consumer reactions based on their income quintile (pp. 86-87). Different consumer reactions to tax increases were also found by Spiller et al. (2017), who found that especially lower income and rural households are more affected by gasoline tax increases (p. 75).  

#< quiz "Consumer Response based on Income Quintile"
question: How does the tax elasticity change with a higher income?
sc:
- The tax elasticity increases with income.* 
- The tax elasticity decreases with income.

success: Great, your answer is correct!
failure: Try again.
#>

As you may have guessed correctly, richer households can afford to adapt their consumer choices to tax increases, thus the demand elasticity rises. They have a bigger financial scope to choose fuel-efficient vehicles when buying new or moving their residence closer to work. You will learn more about the persistent nature of gasoline taxes and its consequences in *Exercise 6.1*.

Davis and Kilian (2011) also estimate the implication of a carbon tax increase with reference to the transportation sector. Their estimates imply that a 10-cent tax increase leads to a decrease of carbon emission in the transport sector by $1,5$% in the short run. The authors also find that their estimation results indicate a higher sensitivity of consumers towards tax changes than to price changes (p.1188). 


*The regression results of the last table can be found in column 7 and 8 of Table 2 on page 313 in the paper.*



## Exercise 4 -- Robustness Check


As we have learned earlier, the explanatory variables need to be exogenous in order to avoid a biased and inconsistent estimator $\hat{\beta}$ for our linear regression estimation. In the previous chapters, we performed linear regressions with additional control variables as well as fixed effects and controlled for clustered standard errors. Still, there is the possibility that the explanatory variables are correlated with further omitted variables in the error term. While this can never be ruled out completely, we can be more confident that an explanatory variable is not endogenous, if it does not correlate with other relevant variables that are included in our data. 

### Exogeneity of Gasoline Taxes


In the following section, we thus investigate whether different socioeconomic and political variables, which are not included in our regression model yet, have a causal effect on the height of gasoline taxes.
 
These variables include:
- socioeconomic variables: 
  + gross state product (GSP) per capita
  + mean family size
  + autos per capita
  + drivers per capita
  + unemployment
  + urban population share
  + population share in a metropolitan state are (MSA) with rail access
  + educational attainment (fraction of high school graduates and fraction of BA graduates)


- political variables: 
  + democrat governor
  + fraction of democrats in state senate
  + fraction of democrats in state house
  + a state's budget surplus as a fraction of revenue

In contrast to the preceding regression models, year-to-year changes in nominal state gasoline taxes per gallon are regressed on the first-differenced value of the potential confounding variables listed above. Hence, the dependent variable changes from the gasoline consumption to the **state gasoline taxes**. 

You can learn more about the reason for and the function of first-differencing (FD) in the *info box* below.

#< info "First-Differencing"

First-differencing is a method, which is used on panel data. Remember that panel data sets contain information about the same cross-sectional units that are followed over time. Due to that, we cannot assume that the observations are independently distributed over time. Instead, there are unobserved factors, which influence the observations of a certain unit over the years. In our case, unobserved factors, which affect a state's gasoline consumption in 1972 might as well affect the consumption in 1980. First-differencing offers a method to remove these time-constant, unobserved attributes of the units being studied.

We add $a_{s}$ to our regression model, which captures all unobserved, time-constant factors that affect the dependent variable $y_{st}$. As $a_{s}$ does not change over time, it has no subscript $t$. $a_{s}$ can be called an unobserved effect and is also often referred to as a fixed effect. The error term $\epsilon_{st}$ is then referred to as the time-varying or idiosyncratic error, as it subsumes all unobserved variables that affect $y_{st}$ and do change over time. In our example, ${s}$ denotes different states. Working with first-differencing is meant to remove a bias that is caused by time-constant variables. It is thus an alternative to the use of a fixed effects estimation. 

Assume, we have two years of panel data and are looking for a consistent estimate of the parameter $\beta_{1}$. We want to estimate the following linear equation: $$y_{st}=\beta_0+\delta_0 d2_{t}+\beta_1 x_{st}+a_{s}+\epsilon_{st}$$

$t$=1,2.

$d2_{t}$ is a dummy variable that equals to zero when $t=1$ and to one when $t=2$. Hence, the intercept for $t=1$ is $\beta_0$ and for $t=0$ is $\beta_0+\delta_0$. 

The unobserved effect $a_{s}$ affects the dependent variable and is correlated with the explanatory variables. As mentioned before, $a_{s}$ is constant over time. Therefore, we can difference the data across the two years. The years can be written as: 

$$y_{s2}=(\beta_0+\delta_0)+\beta_1 x_{s2}+a_{s}+\epsilon_{s2} (t=2)$$

$$y_{s1}=\beta_0+\beta_1 x_{s1}+a_{s}+\epsilon_{s1} (t=1)$$

By subtracting the *second* equation from the *first*, we obtain: 

$$(y_{s2}-y_{s1})=\delta_0+\beta_1(x_{s2}-x_{s1})+(\epsilon_{s2}-\epsilon_{s1})$$

or 

$$\Delta y_{s}=\delta_0+\beta \Delta x_{s}+\Delta \epsilon_{s}$$
$\Delta$ defines the change from $t=1$ to $t=2$. Note, that the unobserved effect $a_{s}$ does not appear in the last equation: it has been "differenced away". Therefore, the equation is called a **first-differenced equation**. As we can see, each variable has been differenced over time. It is important, that $\Delta \epsilon_{s}$ is uncorrelated with $\Delta x_{s}$. In other words, the error term at each time has to be uncorrelated with the explanatory variable in *both* time periods.  This is also called **strict exogeneity** assumption. The resulting OLS estimator for $\beta_1$ is then called the **first-differenced estimator**. 

First-differencing can also be performed with more than two time periods. The equation cannot be analyzed for the first observation, though, as no lagged values exist for it. 

Woolridge (2013) describes this theoretical background in the chapters 13.3., 13.4 and 13.5 from page 459 to 474. 

#>

**Task**: In order to start with our exercise, we load our data set and call `head()` to look at the data frame. Just press *edit* and *check* to see the result. 

```{r "8_1"}
#< task_notest
dat <- readRDS("dat_fd.rds")

head(dat)
#>
```

Again, the data frame `dat` is based on the same data base as in the previous exercises. It consists of $16$ columns, including the state and the year of the observations, our explanatory variable `sgastax` and the possible confounding variables listed at the beginning of this exercise. 

**Task**: Exclude the years 2009 and 2010 with the `filter` command, as there are missing values. Assign the new data frame to the name `sel_dat`. 

```{r "8_2"}
#< task

#>

sel_dat <- dat %>%
  filter(year<2009)

```

We calculate the first-differenced variables by state. As we want to rule out time-constant factors, as explained in the *info box*, it is necessary to further arrange the state data by year. 

**Task:** Assign the new data frame to the variable `dat_fd`. First, arrange the data by `state` and `year`. Then group our data by `state`. Lastly, look at the new data frame with the `head` command. Fill in the blanks in the code chunk. 

```{r "8_3"}
#< fill_in

___ <- sel_dat %>%
  arrange(___, ___, by_group = TRUE)%>%
  ___

___

#>
dat_fd <- sel_dat %>%
  arrange(state, year, by_group = TRUE)%>%
  group_by(state)

head(dat_fd)

#< hint
display("First assign the new data frame to the variable named in the task. Add state and year in the right order in the arrange command. Then, use the group_by command for the state and call head.")
#>
```

Now, that the data is arranged by state and ascending by year, we can calculate our first-differenced variables. As an example: we subtract the value of the mean family size in Alabama of the year 1977 from the mean family size in Alabama in 1978. In general this can be expressed like this: $fd_x=x[n]-x[n-1]$, where $[n-1]$ stands for the value of the same variable, which was one period before. In R, we can easily calculate that with the `lag` command, which we already used in *Exercise 1*.  

**Task:** Just press *check* to calculate the first-differenced variables of our dependent and all explanatory variables with `lag()` and `mutate()`. 

```{r "8_4"}
#< task_notest

dat_fd <- dat_fd %>%
mutate(fd_sgastax=sgastax-lag(sgastax),
       fd_gsp_capita=gsp_capita-lag(gsp_capita), 
       fd_fsize=fsize-lag(fsize),
       fd_autos_capita=autos_capita-lag(autos_capita),
       fd_drivers_capita= drivers_capita-lag(drivers_capita),
       fd_unemployment=unemployment-lag(unemployment),
       fd_urbanization=urbanization-lag(urbanization),
       fd_railpop=railpop-lag(railpop), 
       fd_hsgrad=hsgrad-lag(hsgrad), 
       fd_bagrad=bagrad-lag(bagrad),
       fd_Democrat_Gov= Democrat_Gov-lag(Democrat_Gov),
       fd_demsfrac=demsfrac-lag(demsfrac),
       fd_demhfrac=demhfrac-lag(demhfrac),
       fd_budget_surplus= budget_surplus-lag(budget_surplus))

#>

```

In the paper, the authors perform the regression on lagged, contemporary and leading first-differenced variables. We focus on the lagged first-differenced explanatory variables as the results reach the highest statistical significance of the before mentioned options. 

So now, the lagged first-differences of the explanatory variables are calculated and added to our data frame: $lagfd_x=fd_x[n-1]$. 

**Task:** We calculate the lag of the first-differenced variables of our explanatory variables with the `lag()` and `mutate()` command. Just press *check* to see the result.

```{r "8_5"}
#< task_notest

dat_lag_fd <- dat_fd %>%
  mutate(lag_fd_gsp_capita=lag(fd_gsp_capita),
       lag_fd_fsize=lag(fd_fsize),
       lag_fd_autos_capita=lag(fd_autos_capita),
       lag_fd_drivers_capita= lag(fd_drivers_capita),
       lag_fd_unemployment=lag(fd_unemployment),
       lag_fd_urbanization=lag(fd_urbanization),
       lag_fd_railpop=lag(fd_railpop), 
       lag_fd_hsgrad=lag(fd_hsgrad), 
       lag_fd_bagrad=lag(fd_bagrad), 
       lag_fd_Democrat_Gov=lag(fd_Democrat_Gov),
       lag_fd_demsfrac=lag(fd_demsfrac),
       lag_fd_demhfrac=lag(fd_demhfrac),
       lag_fd_budget_surplus= lag(fd_budget_surplus))

head(dat_lag_fd)

#>

```

Now, our data frame additionally includes the first-differenced and the lagged first-differenced version of our variables. Note, that for the first-differenced columns, there is no value for the first year, as there is no lagged version. The same applies for the lagged first-differenced values, where the first two years have no values. 

We perform a regression of the first-differenced gasoline state tax `fd_sgastax` on lagged first-difference explanatory variables. The explanatory variables are the changes of the socioeconomic and political variables listed at the beginning of this exercise. Moreover, we add a year fixed effect and the standard errors are clustered by state. 

**Task:** Just press *check* to run the regression and show the output with `stargazer`. 

```{r "8_6",results='asis'}
#< task_notest

#Run the regression
reg_fd <- felm(fd_sgastax ~  lag_fd_gsp_capita + lag_fd_fsize + lag_fd_autos_capita+
               lag_fd_drivers_capita + lag_fd_hsgrad+ lag_fd_bagrad+ 
               lag_fd_urbanization + lag_fd_railpop + lag_fd_Democrat_Gov+
               lag_fd_demsfrac+lag_fd_demhfrac+lag_fd_unemployment+
               lag_fd_budget_surplus |year|0|state, data= dat_lag_fd) 


#Output
stargazer(reg_fd, type = "html",
          covariate.labels=c("GSP per capita (000s)", "Mean family size", 
                             "Autos per capita", "Drivers per capita", 
                             "Fraction of adults graduating high school",
                             "Fraction of adults with BA", 
                             "Urban population share", 
                             "Population share in MSA with rail", 
                             "Democrat governor", 
                             "Fraction of Democrats in state senate", 
                             "Fraction of Democrats in state house", 
                             "Unemployment rate", 
                             "Percent state budget surplus"))

#>
```

#< quiz "Significant Regression Coefficient"
question: Which of the regression coefficients is significant at the 1% level or below? Type in the result, exactly as it is named in the table above.
answer: GSP per capita (000s)
#>

As you have probably guessed correctly, we find only one regression coefficient significant at the $1$% level or below: past changes in gross state product per capita (GSP) are negatively correlated with gasoline tax changes with a value of $-0.167$ with a standard deviation of $0.057$. Please note that generally regressing a dependent variable on many explanatory variables simultaneously might lead to higher standard errors.

#< quiz "Interpretation of Regression Coefficient"
question: Please complete the following sentence. An increase of the GSP per capita by 1.000$...

sc:
- decreases the state gasoline tax by 16.7%.
- decreases the state gasoline tax by 0.167$.*

success: Great, your answer is correct!
failure: Try again.
#>

However, the main regression model in the previous exercise controls for GSP with the variable `lnincpop`, which represents the logarithm of the real income per adult in the specific state. Altogether, the variation in gasoline prices cannot be explained by the political and economic variables beyond what is already explained by year fixed effects. The current regression results suggest that tax changes are not correlated with observable socioeconomic and political variables. 

The authors mention a few reasons that explain the exogeneity of gasoline taxes to socioeconomic and political variables. First, there is a time gap between the start of the public debate and the actual tax change. Hence, the potential that tax changes may be timely correlated with other variables affecting gasoline consumption is reduced. Secondly, many tax increases are mostly motivated by the gaps between transportation funding and infrastructure needs. Hence, other variables do not influence the tax increases temporarily as much. Population, long-run economic growth and land development influence the infrastructure needs, which tend to vary gradually over time. In our estimation, we use quadratic time trends that are likely to control for these variables. Third, during a public debate about tax changes the media coverage of the gasoline taxes rises. Both, the current tax level and the planned tax increase are mentioned in the coverage. As a result, the salience of the tax changes is increased. The consequences of the media salience are evaluated in *Exercise 6.2*. Then, we will analyze further whether salience is a possible explanation for the effects of tax changes on the consumer behavior (cf. Li et al. (2014), pp. 316-317).

*These results correspond to the first column of Table 3 of Li et al. on page 317. Note, that due to different rounding, the values differ slightly.* 


## Exercise 5 -- Effects of Gasoline Taxes on Consumer Behavior: Household Data Analysis


So far, we examined the effects of tax-exclusive prices as opposed to tax components on gasoline consumption and found that consumers react more strongly to tax changes than changes in the tax-exclusive prices. Within this exercise, we will examine the vehicle purchase and driving decisions of consumers regarding gasoline price or tax changes.

The basis of our data frame is the 1995, 2001 and 2009 National Household Travel Survey (NHTS) that is conducted by agencies of the Department of Transportation through random sampling. The NHTS provides household-level data on vehicle stocks, travel behavior as well as household demographics. The provided household demographics include income bins, education bins, the number and age of adults, and the numbers of workers and drivers in the household. Moreover, neighborhood information is incorporated: rural and urban indicators, population, working population, housing density and the availability of rail. So there are a number of controls for characteristics which may vary with both a state's tax rate and the household's driving or purchase decisions (cf. Li et al. (2014), pp. 327-328).


## Exercise 5.1  -- The Effects on Vehicle Purchase - MPG 


For the examination of the **effects on vehicle purchase**, we use data on MPG (miles per gallon) for each vehicle. Within the data frame, the make and model of the household's vehicles are matched to the EPA fuel economy database in order to obtain the MPG for each vehicle. The gasoline prices at the time of purchase are based on the gasoline prices used in the aggregate data analysis from the previous exercises. As there is the possibility that purchases of newer vehicles respond differently to price changes than purchases of older vehicles, the analysis is separated in two samples. 

The first sample contains 52,128 observations of households that bought at least one used or new vehicle during the 12 months prior to the survey. The second sample includes 30,363 households, which purchased a vehicle that is less than 4 years old within the 12 months prior to the survey. 

First, we need to load our new data frame. 

**Task:** Just press *edit* and *check* to load the data frame `mgp_forall` and show it with the `head` command. 

```{r "10_1"}
#< task_notest
mpg_forall <- readRDS("mpg_forall.rds")
head(mpg_forall)
#>
```

In our new data frame `mpg_forall`, each row represents one household.
The data frame contains 157 columns. In addition to the already familiar variables such as the tax-exclusive gas price or the gas tax, there are a number of so called dummy variables concerning the division, state, worker density and  population density by census tract, urban and rural indicator variables, rail availability, education of the reference person, household income, month, year and MSA (Metropolitan Statistical Area) size. Those are all categorical variables. You can find out more about dummy variables in the following *info box*. 

#< info "Dummy Variables"

Dummy variables are qualitative variables in the form of binary information. If, for example, a person owns a car or not, this information can be displayed by defining binary variables or a zero-one variable. When the respective person owns a car, the variable takes the value $1$, if not it is $0$ (cf. Woolridge (2013), p. 227). In other words, a dummy variable helps to distinguish between two groups. Consequently, the coefficient of a dummy variable estimates the ceteris paribus difference between the two groups. It is also possible to create dummy variables for more than two groups. Then, there has to be an own dummy variable for each group in the model. If the state a person is living in is supposed to be a dummy variable, we need a column for each state with its own dummy variable. If the household is located in Pennsylvania, for example, the column representing Pennsylvania will contain a $1$ and all other state columns contain a $0$ (cf. Woolridge (2013), p. 257).

#>

Moreover, the shown specifications include a detailed set of household specific controls where non-categorical variables are presented by quadratic functions. After the author's specification this applies to the household size, the age of the reference person and the number of adults, drivers and workers per household (cf. Li et al. (2014), p. 328). 

In order to compare the two samples, we have to load the second sample as well. Thus, we repeat the last two tasks for the data set `mpg_fornew`. 

**Task:** Just press *check* to read in the data frame `mpg_fornew` and use the `head` command to show it. 

```{r "10_2"}
#< task_notest
mpg_fornew <- readRDS("mpg_fornew.rds")
head(mpg_fornew)
#>
```

The included columns are the same as in the previous data frame. Remember, that it only includes households who bought a car newer than 4 years. Hence, the sample is smaller.


### Average of Characteristics of the Different Data Frames


Let's compare the average values of some numeric variables of the two data frames to see how they change when only newer cars are included. With `select`, we choose the same columns out of the two data frames: 
+ the average MPG per household `mpg_all`
+ the household size `HHSIZE`
+ the household income `HHFAMINC`
+ the education of the reference person `REF_EDUC`. 

#< quiz "Average"
question: |
  Take a guess. Do the characteristics described above differ much between the two samples? 
sc:
- yes
- no*
success: Great, your answer is correct!
failure: Try again.
#>

In order to answer this question, we will calculate the weighted means of the variables. We will make use of some very helpful `dplyr` functions again. First, we need to `select` the columns we want to investigate. After that, we use the `summarise_all` function, which creates a new data frame that only contains the results of our computation.  We need to calculate the weighted mean, which has its own function of the same name. In the end, we will round the results in order to get a quicker overview. 

In general, the sampling weights within the data frames ensure that the metrics about the households in our data set are representative of all U.S. households. Clearly, the effort of surveying all households would be too high. By using the sampling weights for calculations, the sample is more reflective of the entire population. 

#< info "weighted.mean()"

The `weighted.mean()` computes the weighted arithmetic mean of a variable. The function needs the name of the variable for which the mean should be calculated and the name of the variable that stores the weights. Possibly, `na.rm` can be added, which indicates if the NA values in the variable will be omitted before the calculation. Assume, we want to calculate the weighted mean for the variable `REF_EDUC`, the weights are stored in a column named `weight` and we want to exclude the missing values. Then, the function would look like this:

```{r "10_3"}
#weighted.mean(REF_EDUC, w=weight, na.rm=TRUE)
```

For more info call `help(weighted.mean)`. 

#>

**Task**: Press *check* in order to get a data frame that stores the weighted and rounded mean values of the above named column variables in the data frame `mean_mpg_forall` and display the generated data. 


```{r "10_4"}
#< task_notest

# Compute the average of each variable 
mean_mpg_forall <- mpg_forall %>%
  select(mpg_all,HHSIZE, HHFAMINC, REF_EDUC)%>%
  summarise_all(weighted.mean, w=mpg_forall$weight, na.rm=TRUE)%>%
  round(2)

#Display the generated data frame
mean_mpg_forall
#>

```


#< quiz "Average percentage difference"
question: |
  Take a guess. Which household specific control variable will have the highest percentage difference between the two samples? 
sc:
- the average MPG per household
- the household size 
- the household income*
- the education of the reference person 

success: Great, your answer is correct!
failure: Try again.
#>

**Task**: Now, perform the exact same calculation for the data frame `mpg_new`. 

```{r "10_5"}
#< task

#>

# Compute the average of each variable
mean_mpg_fornew <- mpg_fornew %>%
  select(mpg_all, HHSIZE, HHFAMINC, REF_EDUC)%>%
  summarise_all(weighted.mean,w=mpg_fornew$weight, na.rm=TRUE)%>%
  round (2)

#Display the generated data frame
mean_mpg_fornew

#< hint

display ("The easiest way is to copy the solution of the code chunk above and replace forall with fornew everywhere it occurs.")

#>
```


Within the two data frames, the average MPG of vehicles is almost equal. The other variables are quite close as well. Remember that the households in sample 2 bought newer vehicles. This aligns with the observations that the household sizes are smaller, the income is higher and the education is slightly higher as well (cf. Li et al. (2014), p. 329).

*These results are part of Table 8, p. 329 in the paper. Due to different rounding, the numbers differ slightly though.*


### Multiple Linear Regression with the Household Data


Following our linear model in *Exercise 3*, we want to examine the effect of gasoline taxation on the vehicle purchase. Again, we are doing so by comparing the causal effect of the tax-exclusive price and the tax component on the vehicle purchase with the following equation: 

$$log(MPG_{it})= \beta_{0}+\beta_{1} log(1+\frac{\tau_{sq}}{p_{sq}})+\beta_{2} log(p_{sq})+X_{i}\Theta_{m}+\delta_{s}+\phi_{t}+\epsilon_{i}   $$

- $i$: a household surveyed in month $t$
- $s$: state
- $q$: quarter
- $MPG_{it}$: average MPG of all new and used vehicles (cars and light trucks) purchased during the last 12 months by household $i$ surveyed in month $t$
- $log(1+\frac{\tau_{sq}}{p_{sq}})$: logarithm of tax ratio which corresponds to the household's state $s$ and quarter of vehicle purchase $q$
- $log(p_{sq})$: tax-exclusive gasoline price which corresponds to the household's state and the quarter of purchase

#< quiz "Causal Effect on MPG"
question: Before performing the regression estimation, complete the following sentence. An increase of the tax component or the tax-exclusive price will have a ... effect on the MPG. 

sc:
- positive*
- negative
success: Great, your answer is correct!
failure: Try again.
#>

Miles per Gallon denote the distance in miles that a vehicle can travel on one gallon of fuel. Hence, a higher MPG value means that the vehicle needs less fuel for the same distance - it is more fuel-efficient. If gasoline prices rise, buying a vehicle with a higher MPG would be the reasonable choice. 

In the following regression, we will use sampling weights again. Thereby, we make sure, that the metrics about the households in our data set are representative for the whole population. As we are using the dummy variables as controls, the formula gets quite long.  

**Task**: We regress the logarithm of the average MPG of all new and used vehicles bought in the last 12 months `lnmpg` on the logarithm of the tax ratio which corresponds to the household's state and quarter of vehicle purchase `lnpurtaxrate11` and the respective logarithm of the tax-exclusive gasoline price `lnpre_purprice11`. The standard errors are clustered by state again. First, we use the larger sample $1$. 
Just press *check* to run the regression. 

```{r "10_6",results='asis'}
#< task_notest
reg_mpg1 <-felm(lnmpg ~lnpurtaxrate11+lnpre_purprice11+HHSIZE+ hhsize2 +DRVRCNT+ drvrcnt2+ ln_age+ ln_age2+ NUMADLT+ numadlt2+ WRKCOUNT+ wrkcount2 + workerden_d1 +workerden_d2+workerden_d3+workerden_d4+workerden_d5+workerden_d6+workerden_d7+workerden_d8+popden_d1+popden_d2+popden_d3+popden_d4+popden_d5+popden_d6+popden_d7+popden_d8+urban_d1+urban_d2+urban_d3+urban_d4+rail_d1+rail_d2+educ_d1+educ_d2+educ_d3+educ_d4+educ_d5+inc_d1+inc_d2+inc_d3+inc_d4+inc_d5+inc_d6+inc_d7+inc_d8+inc_d9+inc_d10+inc_d11+inc_d12+inc_d13+inc_d14+inc_d15+inc_d16+inc_d17+inc_d18+month_d1+month_d2+month_d3+month_d4+month_d5+month_d6+month_d7+month_d8+month_d9+month_d10+month_d11+month_d12+msa_d1+msa_d2+msa_d3+msa_d4+msa_d5+msa_d6+year_d1+year_d2+year_d3+year_d4+state_d1+state_d2+state_d3+state_d4+state_d5+state_d6+state_d7+state_d8+state_d9+state_d10+state_d11+state_d12+state_d13+state_d14+state_d15+state_d16+state_d17+state_d18+state_d19+state_d20+state_d21+state_d22+state_d23+state_d24+state_d25+state_d26+state_d27+state_d28+state_d29+state_d30+state_d31+state_d32+state_d33+state_d34+state_d35+state_d36+state_d37+state_d38+state_d39+state_d40+state_d41+state_d42+state_d43+state_d44+state_d45+state_d46+state_d47+state_d48+state_d49+state_d50+state_d51+state_d52 |0|0|state_code, weights= mpg_forall$weight, data=mpg_forall)

#>
```

**Task**: Now, we perform the same regression with the shorter sample $2$. 
Just press *check* to run the regression. 

```{r "10_7"}
#< task_notest
reg_mpg2 <- felm(lnmpg ~lnpurtaxrate11+lnpre_purprice11+HHSIZE+ hhsize2 +DRVRCNT+ drvrcnt2+ ln_age+ ln_age2+ NUMADLT+ numadlt2+ WRKCOUNT+ wrkcount2 + workerden_d1+ workerden_d2+workerden_d3+workerden_d4+workerden_d5+workerden_d6+workerden_d7+workerden_d8+popden_d1+popden_d2+popden_d3+popden_d4+popden_d5+popden_d6+popden_d7+popden_d8+urban_d1+urban_d2+urban_d3+urban_d4+rail_d1+rail_d2+educ_d1+educ_d2+educ_d3+educ_d4+educ_d5+inc_d1+inc_d2+inc_d3+inc_d4+inc_d5+inc_d6+inc_d7+inc_d8+inc_d9+inc_d10+inc_d11+inc_d12+inc_d13+inc_d14+inc_d15+inc_d16+inc_d17+inc_d18+month_d1+month_d2+month_d3+month_d4+month_d5+month_d6+month_d7+month_d8+month_d9+month_d10+month_d11+month_d12+msa_d1+msa_d2+msa_d3+msa_d4+msa_d5+msa_d6+year_d1+year_d2+year_d3+year_d4+state_d1+state_d2+state_d3+state_d4+state_d5+state_d6+state_d7+state_d8+state_d9+state_d10+state_d11+state_d12+state_d13+state_d14+state_d15+state_d16+state_d17+state_d18+state_d19+state_d20+state_d21+state_d22+state_d23+state_d24+state_d25+state_d26+state_d27+state_d28+state_d29+state_d30+state_d31+state_d32+state_d33+state_d34+state_d35+state_d36+state_d37+state_d38+state_d39+state_d40+state_d41+state_d42+state_d43+state_d44+state_d45+state_d46+state_d47+state_d48+state_d49+state_d50+state_d51+state_d52|0|0|state_code, data=mpg_fornew, weights= mpg_fornew$weight)

#>
```

**Task**: We want to compare the coefficient estimates of the two samples. Thus, we use the `stargazer` command to show the results of `reg_mpg1` and `reg_mpg2`. We only want to keep the coefficients of the `lnpurtaxrate11` and the `lnpre_purprice11`. 

```{r "10_8",results='asis'}
#< fill_in
stargazer(___, ___,  
        title="Gasoline Taxes, Tax-Exclusive Prices and Vehicle MPG", 
        type="html",
        keep=c("___" , "___"),
        covariate.labels=c("log (1+tax ratio)", "log (tax-excl. gas price)"),
        dep.var.labels = c("log of average household MPG"))
#>
stargazer(reg_mpg1, reg_mpg2,  
        title="Gasoline Taxes, Tax-Exclusive Prices and Vehicle MPG", 
        type="html",
        keep=c("lnpre_purprice11" , "lnpurtaxrate11"),
        covariate.labels=c("log(1+tax ratio)", "log(tax-excl. gas price)"),
        dep.var.labels = c("log of average household MPG"))
#< hint
display("In the first two gaps, add the names of the regression models. In the following gaps, add the names of the coefficients, we want to see in the table.")
#>
```

Note, that on the left you see the results of the regression with sample $1$ and on the right with the shorter sample $2$, where the newly acquired vehicles are less than 4 years old. All coefficient estimates are statistically significant at the $1$% level, except for the coefficient estimate of the log of the tax ratio for sample $1$. It is significant at the $5$ % level. For sample $1$, the coefficient of the tax component $\hat{\beta_1}$ is $0.283$ with a standard error of $0.126$ whereas the coefficient of the tax-exclusive price $\hat{\beta_2}$ is at $0.105$ with a standard error of $0.029$. Thus, with an increase of the tax component by $1$%, the MPG of the bought vehicles increases by $0.283$%. With an increase of the tax-exclusive price of $1$%, the MPG only rises by $0.105$% though. 

Looking at sample $2$, the coefficient estimate for the tax component is even higher and lies at $0.409$ with a standard error of $0.147$, compared to the coefficient estimate of the tax-exclusive retail price of $0.135$ with a standard deviation of $0.037$.

It is striking, that in both regressions, the coefficient estimate on the tax ratio variable $\hat\beta_{1}$ is again larger than that on the tax-exclusive gasoline price $\hat\beta_{2}$. This effect is even more pronounced with sample $2$ focusing on newer vehicles. This seems intuitive as the consumers' purchase decisions might be based on the durable good nature of automobiles. Newer vehicles still have a longer remaining lifetime. When thinking about the future costs of an automobile, price changes through taxes are more persistent than fluctuations due to oil price changes for example. Consequently, they have a higher influence on the purchase decision. Vehicles that travel more miles per gallon are the economic or rational choice then (cf. Li et al. (2014), pp. 335-336). We will learn more about that in *Exercise 6.1*.

**Again, consumers are more price elastic to changes in the tax ratio and alter their behavior accordingly. In other words, the tax elasticity is higher than the tax-exclusive price elasticity.** 

Apparently, tax changes play a larger role on the purchase decisions of consumers than equal changes in the tax-exclusive prices. Is there a similar effect on the short-run vehicle driving decisions? This will be examined in the following section. 


## Exercise 5.2  -- The Effects on Vehicle Driving - VMT


For the examination of the effects on vehicle driving, we use data on VMT (vehicle miles traveled) for each household. The capturing of VMT is quite more challenging than the MPG, which is only dependent on the vehicle model. Survey participants in the NHTS 1995 and 2001 were asked to report odometer readings of all their vehicles. The daily VMT was calculated across all vehicles owned by the same household by comparing the odometer readings a few months apart. The average gasoline price was constructed during the odometer reading period based on the date of the odometer readings and weekly state gasoline prices (cf. Li et al. (2014), p. 329).

However, only a subset of the data can be used as not all survey participants reported the second reading and approximately two-thirds of the households in the 1995 and 2001 survey needed to be dropped. Hence, the final VMT data set contains 28,303 observations. As the authors state in the paper, no comparably large sample with odometer readings has been used within previous literature.

Again, we first need to load our new data frame. 

**Task:** Press *edit* and *check* to load the data set called `vmt_odometer.rds` and store it in a variable called `vmt`. 

```{r "11_1"}
#< task_notest
vmt <- readRDS("vmt_odometer.rds")
#>

```

**Task:** Look at the first few rows of the data set using the `head()` command. Use the variable name of our data frame `vmt` as the only argument. 

```{r "11_2"}
#< task

#>
head(vmt)

```

As in *Exercise 5.1* the data frame contains a number of dummy variables to categorize the month, year, division, state, worker density, population density, urbanization, access to rail, education, income and MSA. The data frame counts 139 columns in total. As in the MPG data frame, the household size, the age of the reference person, the number of adults, drivers and workers per household are presented by quadratic functions. The most important difference is that now, the data frame contains the household's sum of average daily vehicle miles traveled and its logarithm instead of the MPG. Again, each row represents one household. 

The equation for the estimation is similar to the one in *Exercise 5.1*, except for the dependent variable being the **logarithmic VMT per household and state** instead of the MPG. On top of that, month fixed effects are constructed to match the travel period. 

$$log(VMT_{it})= \beta_{0}+\beta_{1} log(1+\frac{\tau_{sq}}{p_{sq}})+\beta_{2} log(p_{sq})+X_{i}\Theta_{v}+\delta_{s}+\phi_{t}+\epsilon_{i}   $$

The dependent variable $VMT_{it}$ shows the daily total vehicle miles traveled across all vehicles belonging to household $i$ that was surveyed in month $t$. 

#< quiz "Causal Effect on VMT"
question: Take a guess on the outcome of the regression by completing the following sentence. An increase of the tax component or the tax-exclusive price will have a ... effect on the VMT. 

sc:
- positive
- negative*
success: Great, your answer is correct!
failure: Try again.
#>

Opposed to the MPG value, a reduction in gas consumption can be achieved by limiting the vehicle miles traveled, thus a higher gas price or gas tax should lower the VMT. 

**Task**: We regress the logarithm of the average VMT `lnvmt` on the logarithm of the tax ratio `lntaxrate` and the respective logarithm of the tax-exclusive gasoline price `lnptx_prce`. The standard errors are clustered by state again and we use sampling weights. Just press *check* to run the regression.

```{r "11_3"}
#< task_notest
reg_vmt <- felm(lnvmt ~lntaxrate+lnptx_prce+HHSIZE+ hhsize2 +DRVRCNT+ drvrcnt2+ ln_age+ ln_age2+ NUMADLT+ numadlt2+ WRKCOUNT+ wrkcount2 + workerden_d1+ workerden_d2+ workerden_d3+workerden_d4+workerden_d5+workerden_d6+workerden_d7+workerden_d8+popden_d1+popden_d2+popden_d3+popden_d4+popden_d5+popden_d6+popden_d7+popden_d8+urban_d1+urban_d2+urban_d3+urban_d4+rail_d1+rail_d2+educ_d1+educ_d2+educ_d3+educ_d4+educ_d5+inc_d1+inc_d2+inc_d3+inc_d4+inc_d5+inc_d6+inc_d7+inc_d8+inc_d9+inc_d10+inc_d11+inc_d12+inc_d13+inc_d14+inc_d15+inc_d16+inc_d17+inc_d18+mon1+mon2+mon3+mon4+mon5+mon6+mon7+mon8+mon9+mon10+mon11+mon12+msa_d1+msa_d2+msa_d3+msa_d4+msa_d5+msa_d6+year_d1+year_d2+year_d3+year_d4+state_d1+state_d2+state_d3+state_d4+state_d5+state_d6+state_d7+state_d8+state_d9+state_d10+state_d11+state_d12+state_d13+state_d14+state_d15+state_d16+state_d17+state_d18+state_d19+state_d20+state_d21+state_d22+state_d23+state_d24+state_d25+state_d26+state_d27+state_d28+state_d29+state_d30+state_d31+state_d32+state_d33+state_d34+state_d35|0|0|state_code, weights= vmt$weight_new, data=vmt)

#>
```


**Task**: Press *check* in order to get a table displaying the regression results of `reg_vmt`.

```{r "11_4",results='asis'}
#< task_notest

stargazer(reg_vmt,   
          title="Gasoline Taxes, Tax-Exclusive Prices and Household VMT", 
          type="html",
          keep=c("lntaxrate","lnptx_prce" ),
          dep.var.labels = c("log of daily household VMT"),
          covariate.labels=c("log(1+tax ratio)", "log(tax-excl. gas price)"))
#>
```


As we can see, the coefficient estimates of the tax-exclusive retail price with $-0.446$ and the tax component $-0.593$ are very close, but the one of the tax ratio is higher. The size of the effects, in absolute values, is quite large. However, the results are rather imprecise with standard errors of $0.249$ and $0.809$. Furthermore, the difference is not statistically significant. Nevertheless, it is consistent with our other findings and again shows a bigger effect of the tax-component.

*The shown regression estimation corresponds to Panel B in the third column of Table 12 on page 336 of the paper.*

Li et al. (2014) perform several more regressions with different samples and with different specifications. For example, they also use self-reported VMT data for a second sample. The authors compare the data of the two samples and find that high-VMT households tend to under-report their travel intensity whilst low-VMT households tend to over-report. As the odometer readings are more exact, we focused on the odometer sample. 
Across all specifications and samples, however, the authors find no statistically significant causal effects on VMT from the tax-exclusive gasoline price or gasoline tax (cf. Li et al. (2014), p. 337). 

To conclude *Exercise 5.1. and Exercise 5.2*, we find that **changes in the gasoline taxation** affect the **vehicle purchase decisions** more than tax-exclusive price changes. For the daily driving decisions, we find no such statistically significant effect of the vehicle miles traveled per household. However, in *Exercise 3* we found that the overall gasoline consumption based on state-level data is affected by gasoline tax changes. Changing the fleet fuel economy by purchasing new vehicles with a higher MPG takes a lot of time, though. Are the findings of *Exercise 3* and *Exercise 5* contradicted then?

First, it needs to be kept in mind, that the data of the VMT analysis with the 1995 and 2001 NHTS only considers a short time frame with only 22 tax changes and an average tax change of $2.43$ cents. In comparison, the aggregate state-level analysis contains 488 gasoline tax changes with an average tax change of $3.64$ cents. This means, the tax changes are larger and more common in the state-level analysis. The second difference between the state-level and household-level analyses consists in the fact that the differential effects in the state-level data are based on annual observations while the VMT-data is based on a few months (on average 3 months) between odometer readings. It is likely that the short time effect on the VMT, if it exists, is smaller because the driving behavior cannot be adjusted as fast. Factors such as the housing and work place locations as well as the availability and access to public transport are very difficult to change in the short run. A short run solution to gasoline price or tax changes exists in the reallocation of driving across vehicles in multi-vehicle households. However, this cannot be examined thoroughly in the household-level analysis because of the lack of gasoline tax variations. Still, there is the possibility that the state-level results condense a larger amount of behavior changes due to the gasoline tax price and taxation, which might explain the stronger effect we found in *Exercise 3* (cf. Li et al. (2014), pp. 337-338).


## Exercise 6  -- Explanations of Findings: Persistence and Salience

In the previous chapters we learned that equivalent gasoline price and tax changes yield distinct demand responses. The consumer's response to gasoline tax changes is larger than the response to changing gasoline prices per se. *Exercise 6* delivers the authors' explanations for our findings and is thus divided into two parts. The first part focuses on persistence whereas part two examines salience. 


## Exercise 6.1  -- Persistence


The author's first possible explanation for the larger consumer response to gasoline tax changes is persistence. This explanation goes towards two directions: the persistence of gasoline tax changes as opposed to tax-exclusive price changes and the persistence or durability of vehicles.

First, changes in gasoline prices due to gasoline tax increases are more persistent than market-driven gasoline price changes. One explicit example for this is that the federal gasoline tax in the U.S. has not been elevated since 1993, for almost 30 years. Oil prices, on the contrary, fluctuate on a daily basis and are rather volatile. Consumers thus perceive tax increases as a permanent or long-term price increase. Consequently, tax changes might induce larger behavioral changes (cf. Davis and Kilian (2011), p. 1188). 

Secondly, the effect of the persistence of gasoline tax changes is further enhanced by the durable goods nature of automobiles. If consumers buy a new vehicle, it is usually a decision for several years. One thought process before buying a car might concern the development of future fuel costs. Rising fuel costs due to tax increases might influence the consumers' purchase decisions and lead them to a more fuel-efficient car with a higher MPG. Additionally, consumers may decide earlier as originally planned to replace their old vehicle with a more fuel-efficient one. Tax changes are longer lasting, thus they have an influence on price expectations and long-run decisions with an impact on gasoline consumption (cf. Davis and Kilian (2011), p. 1202; Li et al. (2014), p. 322).

It seems unlikely that the short-run travel behavior, shown through the VMT, is affected by the persistence of tax changes. However, there might also be a long-run reaction to persistent changes. Consumers might adapt their travel mode, for example to carpooling or rail travel if available, or move to avoid long distance rides to work (cf. Li et al. (2014), p. 322).

Within the scope of this problem set, the plausibility of this explanation cannot be formally tested. Nevertheless, in addition to the above mentioned sources, there is evidence supporting the idea in other literature as well. Scott (2012) examined rational habits in gasoline demand and also found consumers to be more responsive to tax-induced price changes than to market-driven price changes. The consumers adapt their habits, such as the commuting distance, their vehicle stock or their trip frequency, accordingly. 


## Exercise 6.2  -- Salience

The second explanation we are going to explore, is that gasoline tax changes may be more salient to consumers than changes in the tax-exclusive price. Within the following problem set, we will examine one plausible reason why the salience of gasoline tax changes is higher: news coverage. Do tax increases receive more media attention than commensurate increases in the tax-exclusive price of gasoline? The basic idea is that consumers respond more to tax changes or tax-exclusive price changes, when the news coverage is higher (cf. Li et al. (2014), p. 323).

In order to examine whether tax changes receive disproportionate levels of media attention, we use data on the coverage of gasoline-related topics in major U.S. newspapers and nightly television news broadcasts. The data on print media includes 15,623 unique articles indexed by LexisNexis as being highly relevant to either gasoline prices or gasoline taxes. The articles were printed in 25 major newspapers from 1985 to 2008. Both, national dailies such as *The New York Times*  or *USA Today* and major regional newspapers such as *Detroit News* or *The Philadelphia Daily* are included in the data set. The information gathered about each article includes the date it was published and the article's word count. Li et al. combined this information for the analysis by summing up the word count of all articles highly relevant to `gasoline prices` or `fuel taxes` in each month. 

The data on nightly television news broadcasts is drawn from the Vanderbilt Television News Archive. It includes 3,926 *ABC*, *NBC*, *CBS*, *FOX* and *CNN* nightly news segments where the word `gasoline` was part of the abstract of the segment during the years 1983 until 2010. Additionally, the date, network and length of the segment are provided in the archive. The television media coverage is measured by the total number of seconds of coverage of gasoline segments with the words `price` or `tax` appearing in the detailed abstract, per month. 


### Plots of Media Coverage 


For a first overview about the data, let's look at some graphs. As you have already learned, we first need to load the new data sets called `newspaper` and `tvnews`.

**Task:** Just press *edit* and *check* to load the data.

```{r "14_1"}
#< task_notest
newspaper <- readRDS("newspaper_data.rds")
tvnews <- readRDS("tvnews_data.rds")
#>
```

**Task:** Look at the new data set `newspaper` with the `head` command. Just press *check*. 

```{r "14_2"}
#< task_notest
head(newspaper)
#>
```

The newspaper data frame consists of 15 columns: the month and year in which the article was printed,  a column that combines month and year, the gasoline word count with imputed dates when missing, the gasoline word count without imputed dates, the fuel tax and gasoline price word count, and the log gasoline word count. Additionally, there are two columns for time trends and a debate dummy for the May 1996 gas tax repeal debate. Furthermore, the data frame includes columns for the gas tax change of the previous and leading three months as well as for the change in the gasoline retail price within previous and following three months. Each row represents the data for one month of newspaper coverage.

**Task:** Look at the new data set `tvnews` with the `head` command. Just press *check*. 

```{r "14_3"}
#< task_notest
head(tvnews)
#>
```

Similarly to the newspaper data frame, the TV news data frame contains the columns month, year and the combination of the latter monthyear. Furthermore, it includes the sum of the length of relevant broadcasts of the biggest three TV channels *ABC*, *CBS* and *NBC* measured in seconds,  as well as the length of the broadcasts about the gas price and gas tax, separately. Additionally, the length of the nightly news segments with *ABC*, *CBS*, *NBS*, *FOX* and *CNN* is displayed. On top of that, columns with the mean values of the total gasoline tax, federal gasoline tax and state gasoline tax are included as well as the tax-inclusive retail price and the values of the gasoline tax in total. Each row represents the data for one month of TV news coverage.

We close a loop to our first exercise and use the `ggplot()` package again with which we can easily visualize the data. First, we create a plot that shows the **monthly total word count of articles in major U.S. newspapers relevant to gasoline prices or taxes**. 

**Task:** Just press *check*. 

```{r "14_4"}
#< task_notest

library(scales)
p1 <-ggplot(data= newspaper, aes(x=monthyear)) +
      geom_line(aes(y=total_length_imputed))+  scale_color_manual(values=c("black"))+
      labs(x="Date", y="Gasoline word count", 
      title="Word Count of Articles related to Gasoline")+
      geom_vline(xintercept = 370, color = "red")+
      geom_vline(xintercept = 403, color = "red")+
      geom_vline(xintercept = 436.5, color = "red")+
      geom_vline(xintercept = 325, color = "blue", linetype="dashed")+
      geom_vline(xintercept = 342, color = "blue", linetype="dashed")+
      geom_vline(xintercept = 388, color = "blue", linetype="dashed")+
      geom_vline(xintercept = 416, color = "blue", linetype="dashed")+ 
      geom_vline(xintercept = 426, color = "blue", linetype="dashed")+
      geom_vline(xintercept = 513, color = "blue", linetype="dashed") +
      scale_y_continuous(labels=label_number(), limits=c(0,150000))+
      scale_x_continuous(breaks = c(300,360,420,480,540),
      labels = c("1985m1", "1990m1", "1995m1", "2000m1","2005m1"),
      limits=c(300,540))+theme_minimal()
#>
```

Before taking a closer look at the outcome, we create a second plot of the **monthly seconds of gasoline-related nightly news coverage on ABC, CBS and NBC**. 

**Task:** Just press *check*. 

```{r "14_5"}
#< task_notest
p2 <-ggplot(data= tvnews, aes(x=monthyear)) +
        geom_line(aes(y=big3_length))+
        scale_color_manual(values=c("black"))+
        labs(x="Date", y="Nightly News Seconds", 
        title="Nightly News Seconds related to Gasoline")+
        geom_vline(xintercept = 370, color = "red")+
        geom_vline(xintercept = 403, color = "red")+
        geom_vline(xintercept = 436.5, color = "red")+
        geom_vline(xintercept = 325, color = "blue", linetype="dashed")+
        geom_vline(xintercept = 342, color = "blue", linetype="dashed")+
        geom_vline(xintercept = 388, color = "blue", linetype="dashed")+
        geom_vline(xintercept = 416, color = "blue", linetype="dashed")+
        geom_vline(xintercept = 426, color = "blue", linetype="dashed")+
        geom_vline(xintercept = 513, color = "blue", linetype="dashed") +
        scale_x_continuous(breaks = c(300,360,420,480,540),
        labels = c("1985m1", "1990m1", "1995m1", "2000m1","2005m1"),
        limits=c(300,540))+theme_minimal()
#>
```


Now, we want to look at both graphs simultaneously with `grid.arrange()` as in *Exercise 1*. 

**Task:** Use the function `grid.arrange` to show the two plots `p1` and `p2` next to each other. 

```{r "14_6",fig.width=10, fig.heigth=5}
#< task

#>
grid.arrange(p1,p2, ncol=2)

#< hint
display("When using grid.arrange you first need to add the plot names, then define the number of columns ncol=___. ")
#>
```

As you can see, vertical lines in red and blue are included into the plots to draw our attention to certain years.  We did so by adding the argument `geom_vline()` to `ggplot`. By specifying the x-intercept, R prints vertical lines into our graph.

The solid red vertical lines represent the federal gas tax increases in 1990 (by 5.1 cpg) and 1993 (by 4.3 cpg) as well as the congressional debate about repealing the 1993 gas tax increase in May 1996. The dashed blue lines highlight months in which tax-exclusive prices were increased by similar amounts as the tax changes (cf. Li et al. (2014), p. 324). 

#< quiz "Salience of tax changes or tax-exclusive price changes"
question: Around which events do you observe higher amplitudes of the media coverage?

sc:
- tax-exclusive price changes
- tax changes*

success: Great, your answer is correct!
failure: Try again.
#>

It is clearly visible, that both print media and TV news coverage, increase noticeably more prior to federal tax changes than equivalent changes in the tax-exclusive price. Around the blue dashed lines, the news coverage is not relevantly increased. Let's pick out an example. Three months before the tax increase in November 1990, represented by the first red line, the word count of articles related to gasoline almost reached $100,000$ words. Before that, the word count was in the range of approximately $10,000$ to $25,000$ words. Similarly, the seconds of TV news coverage related to gasoline reached a peak of approximately $4,500$ seconds before the tax change in 1990. Previously, the news coverage hardly reached a $1000$ seconds of media coverage per month. Looking at the first blue line, representing a similar increase in the tax-exclusive gasoline price in February 1987, there is no relevant increase in media coverage visible. Three months earlier, the print media word count related to gasoline was at only $2,341$ words and there was no relevant nightly news coverage. 

According to the authors, some gas price changes do trigger media attention though. However, they are much larger in magnitude than the covered tax changes. Altogether, tax-exclusive price changes that triggered similar media attention were at least six times larger than the actual federal tax changes (cf. Li et al. (2014), pp. 324-325).


### Regression of Newspaper Media Coverage


After this first overview with a strong hint to the salience of gasoline taxes in the media, we want to formally test this hypothesis by regressing print and television media coverage on the change in `tax-inclusive gasoline prices` and per gallon `gasoline taxes` in the previous three months and following three months after the change. The specification is: 

$$MediaCoverage=\alpha+\beta_{1}*\Delta p_{t,t-3}+\beta_{2}*\Delta p_{t+3,t}+\gamma_{1} * \Delta \tau_{t,t-3}+\gamma_{2}* \Delta \tau_{t+3,t}+f(t)+ \epsilon_{t}$$

with:
- $\Delta p_{t,t-3}$: changes in the average tax-inclusive price over the previous three months
- $\Delta p_{t+3,t}$: changes in the average tax-inclusive price in the upcoming three months
- $\Delta \tau_{t,t-3}$: changes in the average per gallon tax over the previous three months
- $\Delta \tau_{t+3,t}$: changes in the average per gallon tax in the upcoming three months
- $f(t)$: set of time-effects

Remember, that we found strong evidence of full and immediate pass-through of gasoline taxes to retail prices in *Exercise 2*. Therefore, we differ between the news coverage of commensurate changes in the tax-inclusive price with the coefficients $\beta_1$ and $\beta_2$ and the *additional* news coverage following and preceding a change in the gasoline taxes above what would be expected for the overall tax-inclusive retail prices with the coefficients $\gamma_1$ and $\gamma_2$. (cf. Li et al. (2014), p. 326). 

First, we perform a regression with the dependent variable being the newspaper media coverage on gasoline prices or gasoline taxes. A quadratic time trend is included in order to account for the gradual addition of newspapers tracked by LexisNexis.

**Task:** Just press *check* to run the regression with newspaper coverage regarding the gasoline prices or taxes `total_length` as dependent variable. `total_length` is regressed on changes in the average tax-inclusive gas price in the upcoming and previous three months and on changes in the average per gallon tax in the upcoming and previous three months. Additionally, a debate dummy for the May 1996 gas tax repeal debate is added as well as the mentioned time trends. We calculate robust standard errors.

```{r "14_7"}
#< task_notest
newsreg_1 <- lm(total_length ~fd3_taxin_gas_price+fd3_gas_tax_all+
                lead3_taxin_gas_price+ lead3_gas_tax_all+debate_dummy+time+time2,
                data=newspaper)

summary(newsreg_1, robust=TRUE)
#>
```

Additionally, we perform a regression with the logarithm of the media coverage about gasoline prices or taxes: 

$$log(MediaCoverage)=\alpha+\beta_{1}*\Delta p_{t,t-3}+\beta_{2}*\Delta p_{t+3,t}+\gamma_{1} * \Delta \tau_{t,t-3}+\gamma_{2}* \Delta \tau_{t+3,t}+f(t)+ \epsilon_{t}$$

When only using a **logarithmic dependent variable**, we call it a log-level regression. Find out more about the interpretation of the regression estimates in the *info box* below. 

#< info "Log-Level Regression Model"

Assume, we want to estimate the following simple regression model:

$$ log(y)= \beta_0+\beta_1 x+\epsilon$$
Then, the interpretation of the coefficient $\beta_1$ is: 
If the explanatory variable $x$ is changed by one, a change of $y$ by $(100*\beta_1)$ percent is expected. $(100*\beta_1)$ is also sometimes called the semi-elasticity of $y$ with respect to $x$ (cf. Woolridge (2013), p. 44)
#>

**Task:** Just press *check* to run the regression with the log of the newspaper coverage regarding gasoline prices or taxes `log_length` as dependent variable. 


```{r "14_8"}
#< task_notest
newsreg_2 <- lm(log_length ~ fd3_taxin_gas_price+fd3_gas_tax_all+
                lead3_taxin_gas_price+lead3_gas_tax_all+debate_dummy+time+time2,
                data=newspaper)

summary(newsreg_2, robust=TRUE)
#>
```

#< award "Last Regression Completed!"
Congrats, you've completed the last of many regressions of this problem set. You might as well call yourself regression master. 
#>

As we would like to compare our regression results, we will show them again with the help of the `stargazer()` command. 

**Task:** Use the `stargazer()` command to show our two regressions in one table. List the names of the news regressions as arguments. Part of the code is already given. 

```{r "14_9",results='asis'}
#< fill_in
stargazer(___, ___, 
          title="Media Coverage, Gasoline Price and Tax Changes", 
          type="html",
          out="media_reg.html",
          keep=c("fd3_taxin_gas_price", 
                 "fd3_gas_tax_all",
                 "lead3_taxin_gas_price", 
                 "lead3_gas_tax_all",
                 "debate_dummy"),
          covariate.labels=c("Change in gas price precious three months",
                             "Change in gas tax previous three months",
                             "Change in gas price next three months",
                             "Change in gas tax next three months", 
                             "May 1996 gas tax repeal debate"), 
          dep.var.labels = c("All gas", "log(all gas)"))
#>
stargazer(newsreg_1, newsreg_2,
          title="Media Coverage, Gasoline Price and Tax Changes", 
          type="html",
          keep=c("fd3_taxin_gas_price", 
                 "fd3_gas_tax_all", 
                 "lead3_taxin_gas_price", 
                 "lead3_gas_tax_all",
                 "debate_dummy"),
          covariate.labels=c("Change in gas price previous three months", 
                             "Change in gas tax previous three months",
                             "Change in gas price next three months",
                             "Change in gas tax next three months", 
                             "May 1996 gas tax repeal debate"), 
          dep.var.labels = c("All gas", "log(all gas)"))

#< hint
display("Just add the names of the regression models in the gaps.")
#>
```

Note that the `stargazer` results report different standard errors as the `summary` function with robust standard errors.

We find significant differences in the overall news coverage about gasoline price or tax changes dependent on whether there was a change in the overall *gas price* or *gas tax* within the previous three months. The estimation shows that a $0.01$ USD change in the *gas price* three months ago leads to approximately $806$ words of newspaper coverage, whereas a $0.01$ USD change in the *gas tax* is associated with $2,870$ words of additional print media coverage. A *gas tax change* within the upcoming three months is associated with even higher additional news coverage: $3,622$ words whereas the coefficient on the *gasoline price changes* is negative. 

#< quiz "Log-level interpretation print"
question: The coefficient of the gas tax change in the previous three months lies at $0.177$. How can this value be interpreted? Complete the following sentence. With a change of $0.01$ USD of the gas tax, the media coverage increases by ...

sc:
- 177 words.
- 1.77 percent.
- 17.7 percent.*

success: Great, your answer is correct!
failure: Try again.
#>

#< award "Interpretation Expert!"
Congratulations, you got to know different models with and without using logarithms including a level-level model, log-log model and log-level model and are able to interpret them correctly.
#>

Looking at the coefficients of the log-level model, the differences between the news coverage after a change in the gas price versus the gas tax are very pronounced. With a change of $0.01$ USD of the *gas price* in the previous three months, the media coverage only rises by $1,2$%. With a change of $0.01$ USD of the *gas tax*, however, the media coverage increases by $17.7$%. The change of the *gas price* in the next three months, hardly has an effect on the media coverage with $0.5$%. On the contrary, an upcoming change of the *gasoline tax* evokes an estimated increase in the print media coverage of $27.5$%.

**All in all, gas tax changes lead to much greater increases in print media coverage.** 


### Regression of TV News Media Coverage


Does the TV news coverage support these findings as well? 

The authors regress the length of the nightly TV news segments on changes in the average tax-inclusive gas price in the upcoming and previous three months and on changes in the average per gallon tax in the upcoming and previous three months. In our previous formula of the media coverage only the dependent variable changes to the TV news coverage instead of newspapers: 

$$MediaCoverage=\alpha+\beta_{1}*\Delta p_{t,t-3}+\beta_{2}*\Delta p_{t+3,t}+\gamma_{1} * \Delta \tau_{t,t-3}+\gamma_{2}* \Delta \tau_{t+3,t}+f(t)+ \epsilon_{t}$$

As we have seen in the plots, the curves of the print versus TV news coverage run rather similarly. Therefore, we won't perform the linear regression of the TV news coverage. Instead, you are invited to take a guess on the results. Afterwards, the results are presented. Answer them with the knowledge you gained from the print media coverage regression models. 

#< quiz "Tax change in previous or upcoming three months"
question: Check the box with the event that likely evokes the most media attention. 

mc:
- previous price change
- previous tax change
- upcoming price change 
- upcoming tax change*

success: Great, your answer is correct!
failure: Try again.
#>

As you might have guessed, an upcoming tax change provokes the highest media attention. First, let us look at the *gasoline price* induced TV news coverage. A change of $0.01$ USD in the *gas price* within the previous three months leads to $14.05$ seconds of additional nightly news coverage, whilst a $0.01$ USD change within the next three months lowers the nightly news coverage by $11.80$ seconds. A change of $0.01$ USD in the *gas tax* in the previous three months leads to additional airtime of $18.42$ seconds, the same change in the upcoming three months leads to additional news coverage of $129$ seconds. Thus, the coverage of a gas tax change is around 7 times larger before the actual change than afterwards. Furthermore, tax changes again lead to higher additional news coverage. 

For the TV news coverage also a log-level specification is available. As we have learned, the additional media coverage *before tax changes* is the highest. 

#< quiz "Ceteris Paribus effect before tax change"
question: If we compare the additional news coverage three months before the tax change. Would you think the additional coverage in the newspapers or in the nightly TV segments is relatively higher? 

sc:
- newspaper coverage.
- TV news coverage.*

success: Great, your answer is correct!
failure: Try again.
#>

With a value of the semi-elasticity of $0.275$ for the print media coverage versus $0.392$ for the TV news coverage, an upcoming tax change of $0.01$ USD has a larger effect on the TV news coverage. 

#< quiz "Log-level interpretation TV news"
question: The coefficient of the gas tax change in the upcoming three months lies at $0.392$. How can this value be interpreted? Complete the following sentence. With an upcoming change of $0.01$ USD of the gas tax, the TV media coverage increases by ...

sc:
- 392 words.
- 3.92 percent.
- 39.2 percent.*

success: Great, your answer is correct!
failure: Try again.
#>

The remaining semi-elasticizes of the log of the TV news coverage are very similar to the ones of the newspaper coverage. With a change of $0.01$ USD of the *gas price*  in the previous three months, the media coverage only rises by $0,74$%. With a change of $0.01$ USD of the *gas tax*, however, the media coverage increases by $19.3$%. The change of the gas price in the next three months, hardly has an effect on the media coverage with $-1.1$%. 

#< award "Quiz, Quiz, Quiz!"
Congratulations, you solved several quizzes and learned  a lot about the data incidentally. 
#>

*These numbers can be found in column 8 of Table 7 on p. 327 of the paper.*

The coefficient estimates of the linear regression models with the TV news media coverage as dependent variable confirm the results of the newspaper regressions. Gas tax changes within the previous or next three months evoke much greater media coverage than commensurate changes in the overall gasoline price. 
**In sum, we can acknowledge that the salience of gasoline tax changes is elevated by the higher media coverage on gas tax changes.** 


## Exercise 7 -- Conclusion

In this problem set, we sought to determine whether equivalent gasoline tax and price changes yield different demand responses. 

For that, we started by getting to know the data with the help of plots and descriptive statistics in *Exercise 1*. There, it was shown that the state gas taxation varies widely between the U.S. states. In general, the U.S. gas tax is comparably low to other industrial countries. With the ordinary least squares method, we then examined the tax incidence in *Exercise 2* and found that the gasoline tax is borne entirely by the consumers. After that, the causal effects between changes in the gas tax and tax-exclusive price and the demand elasticity of consumers were analyzed with a multiple linear regression model in *Exercise 3*. Through the introduction of different control variables, time trends and fixed effects an omitted variable bias was gradually dissolved. **It was found that indeed, consumers react more strongly to changes in the gasoline tax than to commensurate changes in the tax-exclusive price.** Thereafter, a robustness check exemplary confirmed the exogeneity of the dependent variable gasoline tax in *Exercise 4* and showed that it is, with a high probability, not correlated with factors in the errors term. The higher elasticity of consumers regarding changes in the gasoline tax also occur when looking at household data, as done in *Exercise 5*. Consumers' vehicle purchase decisions with regard to fuel-efficiency are influenced more by higher gasoline taxes than commensurate tax-exclusive price changes. This effect was not found, though, when it comes to effects on the short-run driving behavior. However, households often do not have the capacities to decrease their gasoline consumption in the short run, because the daily driving depends on their location, their daily commute, access to public transportation, etc. 
*Exercise 6* presents explanations for the higher causal effect of the gasoline taxation on consumer behavior. The persistence of tax changes combined with the durable goods nature of vehicles, leads consumers to adapt their behavior and decisions. Furthermore, the salience of gasoline taxes due to higher media coverage was shown through OLS estimations in *Exercise 6.2*. 

Altogether, the main finding shows that increasing gasoline taxes can help to reduce the negative external effects of gasoline consumption. Already small increases in the tax or even public debates about tax increases trigger large media attention and lead consumers to adapt their behavior and to take different transportation decisions. These results are supported by Gimenez-Nadal and Molina (2019) who found that higher gasoline taxes are linked to a reduction of commuting time in general and a shift to alternative modes of transport such as public transportation or physical modes of transport instead of car use (p.2). 

However, higher gasoline taxes represent a high financial burden especially for lower income and rural households. Firstly, these households do not have the means to adjust their travel or purchase decisions as much. Secondly, gasoline taxes are slightly regressive. Low income households often spend a larger portion of their income on gasoline and are thus more affected of a tax increase. Spiller at al. thus recommend to recycle a portion of the tax revenue based on income and location in order to alleviate this outcome (cf. Spiller et al. (2017), pp.88). 

In the future, research on the long-run effects of gasoline tax or price changes on the consumer behavior might be added. Eventually, long run effects on the VMT might deliver different results than to what was presented within this analysis. 


Run the following code chunk to see the awards you earned during the problem set. If you completed all tasks, 9 awards shall be displayed. 


```{r "15_1"}
#< task
awards()
#>
```

## Exercise 8 -- References

### Bibliography:

- EIA (2021a): *Oil and petroleum products explained*.*Oil imports and exports*. https://www.eia.gov/energyexplained/oil-and-petroleum-products/imports-and-exports.php.

- EIA (2021b): *Annual Energy Outlook 2021*. https://www.eia.gov/outlooks/aeo/consumption/sub-topic-02.php.

- EPA (2019): *Sources of Greenhouse Gas Emissions*. https://www.epa.gov/ghgemissions/sources-greenhouse-gas-emissions.

- Davis, L.W. and Kilian, L. (2011): *Estimating the Effect of a Gasoline Tax on Carbon Emissions*. Journal of Applied Econometrics (26): 1187-1214. 

- Gimenez-Nadal, J. I. and Molina, J.A. (2019) : *Green
Commuting and Gasoline Taxes in the United States*. IZA Discussion Papers. No. 12377.
Institute of Labor Economics (IZA). Bonn.

- Heiss, F. (2016): *Using R for Introductory Econometrics*. Dsseldorf.

- Kennedy, P. (2013): *A Guide to Econometrics*. 6th Edition. Malden, MA [i.a.]: Blackwell Publishing.

- Li, S., Linn, J. and Muehlegger, E. (2014): *Gasoline Taxes and Consumer Behavior*. American Economic Journal: Economic Policy 6(4): 302-342. 

- Marion, J. and Muehlegger, E. (2011): *Fuel Tax Incidence and Supply Conditions*. National Bureau of Economic Research. Working paper 16863. 

- McNamee, R. (2003): *Confounding and confounders*. Occupational and Environmental Medicine (60):227-234. https://oem.bmj.com/content/60/3/227.

- OECD (2020): *Consumption Tax Trends 2020*.*VAT/GST and Excise Rates, Trends and Policy Issues*. https://www.oecd-ilibrary.org/sites/152def2d-en/index.html?itemId=/content/publication/152def2d-en.

- Parry, I.W.H., Walls, M. and Harrington, W. (2007): *Automobile Externalities and Policies*. Journal of Economic Literature: 373-399. 

- Perret, J. K. (2019): *Arbeitsbuch zur Statistik fr Wirtschafts- und Sozialwissenschaftler : Theorie, Aufgaben und Lsungen.* Wiesbaden: Springer Gabler.

- Rivers, N. and Schaufele, B. (2012): *Carbon Tax Salience and Gasoline Demand*. Department of Economics, Faculty of Social Sciences, University of Ottawa. Working Paper #1211E.

- Scott, R. (2012): *Rational habits in gasoline demand*. Energy Economics 34(5): 1713-1723. https://doi.org/10.1016/j.eneco.2012.02.007.

- Spiller, E., Stephens, H. M., Chen, Y. (2017): *Understanding the heterogeneous effects of gasoline taxes across income and location*. Resource and Energy Economics (50): 74-90. https://doi.org/10.1016/j.reseneeco.2017.07.002.

- Tiezzi, S. and Verde, S. F.(2016): *Differential Demand Response to Gasoline Taxes and Gasoline Prices in the U.S.*. Resource and Energy Economics (44): 71-91.

- Verbeek, M. (2004): *A Guide to Modern Econometrics*. 4th Edition. Chichester, SXW: Wiley.

- Von Auer, L. (2007): *konometrie. Eine Einfhrung*. 4. Auflage, Berlin Heidelberg: Springer-Verlag.

- Wooldridge, J.M. (2013): *Introductory Econometrics: A Modern Approach*. 6th Edition. Boston, MA [i.a.]: Cengage Learning.

### R-Packages: 

- Arnold, J. B. (2021): ggthemes: Extra Themes, Scales and Geoms for 'ggplot2', R package version 4.2.4 https://cran.r-project.org/web/packages/ggthemes/index.html.

- Auguie, B. (2017): gridExtra: Miscellaneous Functions for "Grid" Graphics, R package version 2.3 http://cran.r-project.org/web/packages/gridExtra/index.html.

- Gaure, S. (2021): lfe. Linear Group Fixed Effects. R package version 2.8-7.1. https://cran.r-project.org/web/packages/lfe/lfe.pdf.

- Hlavac, M. (2018): stargazer. Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://cran.r-project.org/web/packages/stargazer/stargazer.pdf.

- Kranz, S. (2020): RTutor. R Problem Sets with Automatic Test of Solution and Hints. R package version 2020.08.11. https://github.com/skranz/RTutor.

- Lamstein, A. (2020): choroplethr: Simplify the Creation of Choropleth Maps in R. R package version 3.7.0. https://cran.r-project.org/web/packages/choroplethr/index.html.

- R Development Core Team (2021). R. A language and environment for statistical computing, R version 4.1.0, R Foundation for Statistical Computing, Vienna, Austria. http://www.r-project.org.

- Stephens, J., Simonov, K. (2020): yaml: Methods to Convert R Data to YAML and Back. R package version 2.2.1. https://cran.r-project.org/web/packages/yaml/index.html.

- Wickham, H. (2021): forcats: Tools for Working with Categorical Variables (Factors). R package version 0.5.1. https://cloud.r-project.org/web/packages/forcats/index.html.

- Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., Dunnington, D. (2021): ggplot2. Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.3.5. https://cran.r-project.org/web/packages/ggplot2/index.html.

- Wickham, H., Francois, R., Henry, L. and Mller, K. (2021): dplyr. A Grammar of Data Manipulation. R package version 1.0.7. https://cran.r-project.org/web/packages/dplyr/dplyr.pdf.

- Wickham, H., Seidel, D. (2020): scales: Scale Functions for Visualization. R package version 1.1.1. https://cran.r-project.org/web/packages/scales/index.html.

All web sources were last retrieved on 11/17/2021. 
